{"file_contents":{"trading/ensemble_analyzer.py":{"content":"import pandas as pd\nimport numpy as np\nimport talib\nfrom datetime import datetime, timedelta\nimport logging\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport joblib\nimport os\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass EnsembleAnalyzer:\n    \"\"\"Institutional-Grade 5-Model Ensemble with Quantitative Finance Methods\"\"\"\n    \n    def __init__(self):\n        self.models = {\n            'sma_crossover': self._sma_crossover,\n            'rsi': self._rsi_analysis,\n            'macd': self._macd_analysis,\n            'bollinger_bands': self._bollinger_bands,\n            'price_momentum': self._price_momentum\n        }\n        \n        # Intraday timeframe weights optimized for 5m-15m\n        self.timeframe_weights = {\n            '5m': 0.40,\n            '15m': 0.35,\n            '1h': 0.25\n        }\n        \n        # Model performance tracking for Sharpe/Sortino weighting (with error handling)\n        try:\n            self.model_performance = self._load_model_performance()\n        except Exception as e:\n            logger.warning(f\"Error loading model performance, using defaults: {str(e)}\")\n            self.model_performance = {\n                model_name: {'weight': 0.2, 'sharpe_ratio': 0.0, 'sortino_ratio': 0.0}\n                for model_name in self.models.keys()\n            }\n        \n        # Probability calibrator (with safe initialization)\n        self.calibrator = None\n        self.meta_classifier = None\n        try:\n            self.scaler = StandardScaler()\n        except Exception as e:\n            logger.warning(f\"Error initializing scaler: {str(e)}\")\n            self.scaler = None\n        \n        # Expected Value tracking\n        self.ev_tracker = {\n            'total_trades': 0,\n            'wins': 0,\n            'losses': 0,\n            'avg_win': 0.0,\n            'avg_loss': 0.0,\n            'win_rate': 0.0\n        }\n        \n        # Dynamic confidence thresholds based on ADX\n        self.confidence_thresholds = {\n            'strong_trend': {'adx_min': 40, 'threshold': 0.8},\n            'moderate_trend': {'adx_min': 25, 'threshold': 0.6},\n            'weak_trend': {'adx_min': 0, 'threshold': 0.4}\n        }\n        \n        # Load or initialize calibration models with error handling\n        try:\n            self._load_calibration_models()\n        except Exception as e:\n            logger.warning(f\"Error loading calibration models, using defaults: {str(e)}\")\n            self.calibrator = None\n            self.meta_classifier = None\n    \n    def analyze(self, data, multi_timeframe_data=None):\n        \"\"\"\n        Institutional-grade analysis with Sharpe weighting, probability calibration, \n        EV filtering, and meta-labeling\n        \n        Args:\n            data (pandas.DataFrame): Primary OHLCV data (5m-15m optimized)\n            multi_timeframe_data (dict): Optional multi-timeframe data\n        \n        Returns:\n            dict: Advanced analysis with quantitative finance metrics\n        \"\"\"\n        try:\n            if data is None or data.empty or len(data) < 50:\n                return self._default_analysis()\n            \n            # Ensure we have enough data\n            data = data.tail(200)  # Use more data for better analysis\n            \n            # Market Regime Detection\n            market_regime = self._detect_market_regime(data)\n            \n            # ATR Calculation for dynamic SL/TP\n            atr_value = self._calculate_atr(data)\n            \n            # Price Action Analysis\n            price_action = self._analyze_price_action(data)\n            \n            # Multi-timeframe analysis if data provided\n            timeframe_consensus = self._analyze_multi_timeframe(data, multi_timeframe_data)\n            \n            # Sharpe/Sortino-weighted ensemble analysis\n            votes = {}\n            details = {}\n            model_weights = self._calculate_model_weights(data)\n            \n            # Run each model with performance tracking\n            for model_name, model_func in self.models.items():\n                try:\n                    vote, detail = model_func(data)\n                    votes[model_name] = vote\n                    details[model_name] = detail\n                    # Update model performance for weight calculation\n                    self._update_model_performance(model_name, vote, data)\n                except Exception as e:\n                    logger.warning(f\"Error in {model_name}: {str(e)}\")\n                    votes[model_name] = 0\n                    details[model_name] = {'error': str(e)}\n            \n            # Calculate Sharpe-weighted ensemble results\n            weighted_signal = self._calculate_weighted_ensemble(votes, model_weights)\n            raw_probability = self._calculate_raw_probability(votes, model_weights)\n            \n            # Probability calibration\n            calibrated_probability = self._calibrate_probability(data, raw_probability)\n            \n            # Expected Value calculation\n            ev_score = self._calculate_expected_value(calibrated_probability, market_regime)\n            \n            # Convert to buy/sell probabilities\n            if weighted_signal > 0:\n                buy_probability = calibrated_probability * 100\n                sell_probability = (1 - calibrated_probability) * 100\n            else:\n                buy_probability = (1 - calibrated_probability) * 100\n                sell_probability = calibrated_probability * 100\n            \n            # Dynamic confidence thresholds based on ADX\n            dynamic_threshold = self._get_dynamic_threshold(market_regime)\n            \n            # Meta-labeling decision\n            meta_decision = self._meta_labeling_decision(data, calibrated_probability, market_regime)\n            \n            # Apply regime-aware signal determination with EV filter\n            final_signal, confidence = self._determine_institutional_signal(\n                weighted_signal, calibrated_probability, market_regime, \n                timeframe_consensus, price_action, ev_score, dynamic_threshold, meta_decision\n            )\n            \n            # Fractional Kelly position sizing\n            kelly_fraction = self._calculate_kelly_fraction(calibrated_probability, ev_score)\n            \n            return {\n                'final_signal': final_signal,\n                'confidence': confidence,\n                'buy_probability': round(buy_probability, 1),\n                'sell_probability': round(sell_probability, 1),\n                'model_votes': votes,\n                'model_details': details,\n                'model_weights': model_weights,\n                'vote_counts': {\n                    'buy': sum(1 for vote in votes.values() if vote == 1),\n                    'sell': sum(1 for vote in votes.values() if vote == -1),\n                    'neutral': sum(1 for vote in votes.values() if vote == 0)\n                },\n                'market_regime': market_regime,\n                'atr_value': atr_value,\n                'price_action': price_action,\n                'timeframe_consensus': timeframe_consensus,\n                'weighted_signal': weighted_signal,\n                'calibrated_probability': calibrated_probability,\n                'expected_value': ev_score,\n                'kelly_fraction': kelly_fraction,\n                'dynamic_threshold': dynamic_threshold,\n                'meta_decision': meta_decision\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error in enhanced ensemble analysis: {str(e)}\")\n            return self._default_analysis()\n    \n    def _sma_crossover(self, data):\n        \"\"\"Moving Average Crossover Strategy (SMA15 vs SMA40)\"\"\"\n        try:\n            close_prices = data['Close'].values\n            sma_15 = talib.SMA(close_prices, timeperiod=15)\n            sma_40 = talib.SMA(close_prices, timeperiod=40)\n            \n            current_15 = sma_15[-1]\n            current_40 = sma_40[-1]\n            prev_15 = sma_15[-2]\n            prev_40 = sma_40[-2]\n            \n            # Check for crossover\n            if prev_15 <= prev_40 and current_15 > current_40:\n                return 1, {'signal': 'Golden Cross', 'sma_15': current_15, 'sma_40': current_40}\n            elif prev_15 >= prev_40 and current_15 < current_40:\n                return -1, {'signal': 'Death Cross', 'sma_15': current_15, 'sma_40': current_40}\n            else:\n                return 0, {'signal': 'No Cross', 'sma_15': current_15, 'sma_40': current_40}\n                \n        except Exception as e:\n            return 0, {'error': str(e)}\n    \n    def _rsi_analysis(self, data):\n        \"\"\"RSI Analysis with 35/65 levels\"\"\"\n        try:\n            close_prices = data['Close'].values\n            rsi = talib.RSI(close_prices, timeperiod=14)\n            current_rsi = rsi[-1]\n            \n            if current_rsi < 35:\n                return 1, {'rsi': current_rsi, 'signal': 'Oversold'}\n            elif current_rsi > 65:\n                return -1, {'rsi': current_rsi, 'signal': 'Overbought'}\n            else:\n                return 0, {'rsi': current_rsi, 'signal': 'Neutral'}\n                \n        except Exception as e:\n            return 0, {'error': str(e)}\n    \n    def _macd_analysis(self, data):\n        \"\"\"MACD Analysis (12,26,9) with histogram\"\"\"\n        try:\n            close_prices = data['Close'].values\n            macd, macd_signal, macd_hist = talib.MACD(close_prices, fastperiod=12, slowperiod=26, signalperiod=9)\n            \n            current_macd = macd[-1]\n            current_signal = macd_signal[-1]\n            current_hist = macd_hist[-1]\n            prev_hist = macd_hist[-2]\n            \n            # Check for MACD crossover and histogram direction\n            if current_macd > current_signal and current_hist > prev_hist:\n                return 1, {'macd': current_macd, 'signal': current_signal, 'histogram': current_hist, 'signal': 'Bullish'}\n            elif current_macd < current_signal and current_hist < prev_hist:\n                return -1, {'macd': current_macd, 'signal': current_signal, 'histogram': current_hist, 'signal': 'Bearish'}\n            else:\n                return 0, {'macd': current_macd, 'signal': current_signal, 'histogram': current_hist, 'signal': 'Neutral'}\n                \n        except Exception as e:\n            return 0, {'error': str(e)}\n    \n    def _bollinger_bands(self, data):\n        \"\"\"Bollinger Bands Analysis (20,2)\"\"\"\n        try:\n            close_prices = data['Close'].values\n            upper, middle, lower = talib.BBANDS(close_prices, timeperiod=20, nbdevup=2, nbdevdn=2)\n            \n            current_price = close_prices[-1]\n            current_upper = upper[-1]\n            current_lower = lower[-1]\n            current_middle = middle[-1]\n            \n            # Calculate position within bands\n            band_width = current_upper - current_lower\n            price_position = (current_price - current_lower) / band_width\n            \n            if current_price <= current_lower:\n                return 1, {'price': current_price, 'lower': current_lower, 'position': price_position, 'signal': 'Below Lower Band'}\n            elif current_price >= current_upper:\n                return -1, {'price': current_price, 'upper': current_upper, 'position': price_position, 'signal': 'Above Upper Band'}\n            else:\n                return 0, {'price': current_price, 'middle': current_middle, 'position': price_position, 'signal': 'Within Bands'}\n                \n        except Exception as e:\n            return 0, {'error': str(e)}\n    \n    def _price_momentum(self, data):\n        \"\"\"5-period Price Momentum\"\"\"\n        try:\n            close_prices = data['Close'].values\n            momentum = talib.MOM(close_prices, timeperiod=5)\n            current_momentum = momentum[-1]\n            \n            # Normalize momentum as percentage\n            current_price = close_prices[-1]\n            momentum_pct = (current_momentum / current_price) * 100\n            \n            if momentum_pct > 0.1:  # Positive momentum threshold\n                return 1, {'momentum': current_momentum, 'momentum_pct': momentum_pct, 'signal': 'Positive'}\n            elif momentum_pct < -0.1:  # Negative momentum threshold\n                return -1, {'momentum': current_momentum, 'momentum_pct': momentum_pct, 'signal': 'Negative'}\n            else:\n                return 0, {'momentum': current_momentum, 'momentum_pct': momentum_pct, 'signal': 'Neutral'}\n                \n        except Exception as e:\n            return 0, {'error': str(e)}\n    \n    def _detect_market_regime(self, data):\n        \"\"\"Detect market regime using ADX and Bollinger Band Width\"\"\"\n        try:\n            close_prices = data['Close'].values\n            high_prices = data['High'].values\n            low_prices = data['Low'].values\n            \n            # ADX calculation for trend strength\n            adx = talib.ADX(high_prices, low_prices, close_prices, timeperiod=14)\n            current_adx = adx[-1]\n            \n            # Bollinger Band Width for volatility\n            upper, middle, lower = talib.BBANDS(close_prices, timeperiod=20, nbdevup=2, nbdevdn=2)\n            band_width = (upper[-1] - lower[-1]) / middle[-1] * 100\n            \n            # Classify market regime\n            if current_adx > 25:\n                regime = 'TRENDING'\n            elif band_width < 2.0:  # Low volatility threshold\n                regime = 'RANGING'\n            else:\n                regime = 'VOLATILE'\n            \n            return {\n                'regime': regime,\n                'adx': round(current_adx, 1),\n                'band_width': round(band_width, 2),\n                'trend_strength': 'Strong' if current_adx > 40 else 'Moderate' if current_adx > 25 else 'Weak'\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error in market regime detection: {str(e)}\")\n            return {'regime': 'UNKNOWN', 'adx': 0, 'band_width': 0, 'trend_strength': 'Unknown'}\n    \n    def _calculate_atr(self, data, period=14):\n        \"\"\"Calculate Average True Range for dynamic SL/TP\"\"\"\n        try:\n            high_prices = data['High'].values\n            low_prices = data['Low'].values\n            close_prices = data['Close'].values\n            \n            atr = talib.ATR(high_prices, low_prices, close_prices, timeperiod=period)\n            current_atr = atr[-1]\n            \n            # Calculate ATR-based levels\n            current_price = close_prices[-1]\n            \n            return {\n                'value': round(current_atr, 5),\n                'sl_multiplier': 1.5,\n                'tp_multiplier': 3.0,\n                'buy_sl': round(current_price - (current_atr * 1.5), 5),\n                'buy_tp': round(current_price + (current_atr * 3.0), 5),\n                'sell_sl': round(current_price + (current_atr * 1.5), 5),\n                'sell_tp': round(current_price - (current_atr * 3.0), 5)\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error calculating ATR: {str(e)}\")\n            return {'value': 0, 'sl_multiplier': 1.5, 'tp_multiplier': 3.0}\n    \n    def _analyze_price_action(self, data):\n        \"\"\"Analyze price action for support/resistance and patterns\"\"\"\n        try:\n            close_prices = data['Close'].values\n            high_prices = data['High'].values\n            low_prices = data['Low'].values\n            \n            # Support and resistance levels (20-period high/low)\n            resistance = np.max(high_prices[-20:])\n            support = np.min(low_prices[-20:])\n            current_price = close_prices[-1]\n            \n            # Distance to key levels\n            distance_to_resistance = abs(current_price - resistance) / current_price * 100\n            distance_to_support = abs(current_price - support) / current_price * 100\n            \n            # Check if near key levels (within 0.2%)\n            near_resistance = distance_to_resistance < 0.2\n            near_support = distance_to_support < 0.2\n            \n            # Doji pattern detection (simple version)\n            open_prices = data['Open'].values\n            last_open = open_prices[-1]\n            last_close = close_prices[-1]\n            last_high = high_prices[-1]\n            last_low = low_prices[-1]\n            \n            body_size = abs(last_close - last_open)\n            candle_range = last_high - last_low\n            \n            # Doji if body is less than 10% of total range\n            is_doji = (body_size / candle_range) < 0.1 if candle_range > 0 else False\n            \n            return {\n                'support': round(support, 5),\n                'resistance': round(resistance, 5),\n                'distance_to_support': round(distance_to_support, 2),\n                'distance_to_resistance': round(distance_to_resistance, 2),\n                'near_key_level': bool(near_resistance or near_support),\n                'doji_pattern': bool(is_doji),\n                'price_position': 'Near Resistance' if near_resistance else 'Near Support' if near_support else 'Clear'\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error in price action analysis: {str(e)}\")\n            return {\n                'support': 0, 'resistance': 0, 'near_key_level': False, \n                'doji_pattern': False, 'price_position': 'Unknown'\n            }\n    \n    def _analyze_multi_timeframe(self, data_1h, multi_timeframe_data=None):\n        \"\"\"Analyze multiple timeframes for consensus\"\"\"\n        try:\n            if not multi_timeframe_data:\n                # If no multi-timeframe data, just analyze 1H\n                return {'consensus': 'NEUTRAL', 'agreement': 33, 'timeframe_signals': {'1H': 'NEUTRAL'}}\n            \n            timeframe_signals = {}\n            \n            # Analyze each timeframe\n            for tf, tf_data in multi_timeframe_data.items():\n                if tf_data is not None and not tf_data.empty and len(tf_data) >= 50:\n                    # Run simplified analysis on each timeframe\n                    votes = {}\n                    for model_name, model_func in self.models.items():\n                        try:\n                            vote, _ = model_func(tf_data.tail(100))\n                            votes[model_name] = vote\n                        except:\n                            votes[model_name] = 0\n                    \n                    buy_votes = sum(1 for vote in votes.values() if vote == 1)\n                    sell_votes = sum(1 for vote in votes.values() if vote == -1)\n                    \n                    if buy_votes >= 3:\n                        timeframe_signals[tf] = 'BUY'\n                    elif sell_votes >= 3:\n                        timeframe_signals[tf] = 'SELL'\n                    else:\n                        timeframe_signals[tf] = 'NEUTRAL'\n                else:\n                    timeframe_signals[tf] = 'NEUTRAL'\n            \n            # Calculate weighted consensus\n            weighted_score = 0\n            for tf, signal in timeframe_signals.items():\n                weight = self.timeframe_weights.get(tf, 0)\n                if signal == 'BUY':\n                    weighted_score += weight\n                elif signal == 'SELL':\n                    weighted_score -= weight\n            \n            # Determine consensus\n            if weighted_score > 0.3:\n                consensus = 'BUY'\n            elif weighted_score < -0.3:\n                consensus = 'SELL'\n            else:\n                consensus = 'NEUTRAL'\n            \n            # Calculate agreement percentage\n            buy_count = sum(1 for signal in timeframe_signals.values() if signal == 'BUY')\n            sell_count = sum(1 for signal in timeframe_signals.values() if signal == 'SELL')\n            total_timeframes = len(timeframe_signals)\n            \n            agreement = max(buy_count, sell_count) / total_timeframes * 100\n            \n            return {\n                'consensus': consensus,\n                'agreement': round(agreement, 0),\n                'weighted_score': round(weighted_score, 2),\n                'timeframe_signals': timeframe_signals\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error in multi-timeframe analysis: {str(e)}\")\n            return {'consensus': 'NEUTRAL', 'agreement': 0, 'timeframe_signals': {}}\n    \n    def _determine_enhanced_signal(self, buy_prob, sell_prob, market_regime, timeframe_consensus, price_action):\n        \"\"\"Determine final signal with enhanced filtering\"\"\"\n        try:\n            # Base signal from original ensemble\n            if buy_prob >= 60:\n                base_signal = 'BUY'\n            elif sell_prob >= 60:\n                base_signal = 'SELL'\n            else:\n                base_signal = 'HOLD'\n            \n            # Start with base confidence\n            confidence = max(buy_prob, sell_prob)\n            \n            # Apply market regime filter\n            if market_regime['regime'] == 'TRENDING' and market_regime['adx'] > 30:\n                confidence += 10  # Boost confidence in strong trends\n            elif market_regime['regime'] == 'RANGING':\n                confidence -= 15  # Reduce confidence in ranging markets\n            \n            # Apply timeframe consensus filter\n            if timeframe_consensus['consensus'] == base_signal:\n                confidence += 15  # Boost for timeframe agreement\n            elif timeframe_consensus['consensus'] != 'NEUTRAL' and timeframe_consensus['consensus'] != base_signal:\n                confidence -= 20  # Reduce for timeframe disagreement\n                base_signal = 'HOLD'  # Override signal if strong disagreement\n            \n            # Apply price action filter\n            if price_action['near_key_level']:\n                confidence -= 10  # Reduce confidence near key levels\n                if confidence < 50:  # If too risky, hold\n                    base_signal = 'HOLD'\n            \n            if price_action['doji_pattern']:\n                confidence -= 5  # Slight reduction for indecision patterns\n            \n            # Cap confidence at 100%\n            confidence = min(100, max(0, confidence))\n            \n            # Final signal determination\n            if confidence < 50:\n                final_signal = 'HOLD'\n            else:\n                final_signal = base_signal\n            \n            # Add confirmation level\n            if confidence >= 80:\n                status = 'CONFIRMED'\n            elif confidence >= 65:\n                status = 'LIKELY'\n            elif confidence >= 50:\n                status = 'POSSIBLE'\n            else:\n                status = 'WEAK'\n            \n            return final_signal, {\n                'percentage': round(confidence, 1),\n                'status': status,\n                'factors': {\n                    'base_ensemble': f\"{max(buy_prob, sell_prob)}%\",\n                    'market_regime': market_regime['regime'],\n                    'timeframe_agreement': f\"{timeframe_consensus['agreement']}%\",\n                    'price_action': price_action['price_position']\n                }\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error in enhanced signal determination: {str(e)}\")\n            return 'HOLD', {'percentage': 0, 'status': 'ERROR', 'factors': {}}\n    \n    def _load_model_performance(self):\n        \"\"\"Load or initialize model performance tracking for Sharpe/Sortino weighting\"\"\"\n        try:\n            if os.path.exists('model_performance.json'):\n                import json\n                with open('model_performance.json', 'r') as f:\n                    return json.load(f)\n        except Exception as e:\n            logger.warning(f\"Error loading model performance: {str(e)}\")\n        \n        # Initialize performance tracking for each model\n        return {\n            model_name: {\n                'returns': [],\n                'sharpe_ratio': 0.0,\n                'sortino_ratio': 0.0,\n                'weight': 0.2,  # Equal weight initially\n                'total_trades': 0,\n                'winning_trades': 0\n            } for model_name in self.models.keys()\n        }\n    \n    def _load_calibration_models(self):\n        \"\"\"Load or initialize probability calibration models\"\"\"\n        try:\n            if os.path.exists('calibration_model.joblib'):\n                self.calibrator = joblib.load('calibration_model.joblib')\n            if os.path.exists('meta_classifier.joblib'):\n                self.meta_classifier = joblib.load('meta_classifier.joblib')\n            if os.path.exists('scaler.joblib'):\n                self.scaler = joblib.load('scaler.joblib')\n                \n            # Load EV tracker\n            if os.path.exists('ev_tracker.json'):\n                import json\n                with open('ev_tracker.json', 'r') as f:\n                    self.ev_tracker = json.load(f)\n        except Exception as e:\n            logger.warning(f\"Error loading calibration models: {str(e)}\")\n    \n    def _calculate_model_weights(self, data):\n        \"\"\"Calculate Sharpe/Sortino-based weights for each model\"\"\"\n        try:\n            weights = {}\n            total_weight = 0\n            \n            for model_name, performance in self.model_performance.items():\n                # Calculate Sharpe ratio weight\n                sharpe = performance.get('sharpe_ratio', 0.0)\n                sortino = performance.get('sortino_ratio', 0.0)\n                \n                # Combine Sharpe and Sortino with preference for Sortino\n                combined_score = 0.4 * sharpe + 0.6 * sortino\n                \n                # Convert to positive weight (add 1 to handle negative ratios)\n                weight = max(0.1, 1 + combined_score)  # Minimum weight of 0.1\n                weights[model_name] = weight\n                total_weight += weight\n            \n            # Normalize weights to sum to 1\n            if total_weight > 0:\n                for model_name in weights:\n                    weights[model_name] /= total_weight\n            else:\n                # Equal weights if no performance data\n                equal_weight = 1.0 / len(self.models)\n                weights = {model_name: equal_weight for model_name in self.models.keys()}\n            \n            return weights\n        except Exception as e:\n            logger.warning(f\"Error calculating model weights: {str(e)}\")\n            equal_weight = 1.0 / len(self.models)\n            return {model_name: equal_weight for model_name in self.models.keys()}\n    \n    def _update_model_performance(self, model_name, vote, data):\n        \"\"\"Update model performance for Sharpe/Sortino calculation\"\"\"\n        try:\n            if model_name not in self.model_performance:\n                return\n                \n            # Simulate return based on vote and actual price movement\n            if len(data) >= 2:\n                current_price = data['Close'].iloc[-1]\n                prev_price = data['Close'].iloc[-2]\n                actual_return = (current_price - prev_price) / prev_price\n                \n                # Calculate model return based on vote alignment\n                if vote == 1:  # Buy signal\n                    model_return = actual_return\n                elif vote == -1:  # Sell signal\n                    model_return = -actual_return\n                else:  # Hold signal\n                    model_return = 0\n                \n                # Update returns list (keep last 100 returns for rolling calculation)\n                returns = self.model_performance[model_name]['returns']\n                returns.append(model_return)\n                if len(returns) > 100:\n                    returns.pop(0)\n                \n                # Calculate Sharpe and Sortino ratios\n                if len(returns) >= 10:\n                    returns_array = np.array(returns)\n                    mean_return = np.mean(returns_array)\n                    std_return = np.std(returns_array)\n                    \n                    # Sharpe ratio\n                    if std_return > 0:\n                        self.model_performance[model_name]['sharpe_ratio'] = mean_return / std_return\n                    \n                    # Sortino ratio (downside deviation)\n                    negative_returns = returns_array[returns_array < 0]\n                    if len(negative_returns) > 0:\n                        downside_deviation = np.std(negative_returns)\n                        if downside_deviation > 0:\n                            self.model_performance[model_name]['sortino_ratio'] = mean_return / downside_deviation\n                \n                # Update trade counts\n                self.model_performance[model_name]['total_trades'] += 1\n                if model_return > 0:\n                    self.model_performance[model_name]['winning_trades'] += 1\n                    \n        except Exception as e:\n            logger.warning(f\"Error updating model performance for {model_name}: {str(e)}\")\n    \n    def _calculate_weighted_ensemble(self, votes, weights):\n        \"\"\"Calculate weighted ensemble signal using Sharpe/Sortino weights\"\"\"\n        try:\n            weighted_signal = 0\n            for model_name, vote in votes.items():\n                weight = weights.get(model_name, 0.2)\n                weighted_signal += vote * weight\n            return weighted_signal\n        except Exception as e:\n            logger.warning(f\"Error calculating weighted ensemble: {str(e)}\")\n            return 0\n    \n    def _calculate_raw_probability(self, votes, weights):\n        \"\"\"Calculate raw probability before calibration\"\"\"\n        try:\n            weighted_signal = self._calculate_weighted_ensemble(votes, weights)\n            # Convert weighted signal to probability using sigmoid-like function\n            probability = 1 / (1 + np.exp(-2 * weighted_signal))\n            return probability\n        except Exception as e:\n            logger.warning(f\"Error calculating raw probability: {str(e)}\")\n            return 0.5\n    \n    def _calibrate_probability(self, data, raw_probability):\n        \"\"\"Calibrate probability using trained logistic regression\"\"\"\n        try:\n            if self.calibrator is None:\n                # If no calibrator trained yet, return raw probability\n                return raw_probability\n            \n            # Extract features for calibration\n            features = self._extract_calibration_features(data)\n            if features is not None:\n                features_scaled = self.scaler.transform([features])\n                calibrated_prob = self.calibrator.predict_proba(features_scaled)[0][1]\n                return calibrated_prob\n            \n            return raw_probability\n        except Exception as e:\n            logger.warning(f\"Error calibrating probability: {str(e)}\")\n            return raw_probability\n    \n    def _extract_calibration_features(self, data):\n        \"\"\"Extract features for probability calibration\"\"\"\n        try:\n            if len(data) < 20:\n                return None\n                \n            close_prices = data['Close'].values\n            high_prices = data['High'].values\n            low_prices = data['Low'].values\n            \n            # Technical indicators as features\n            rsi = talib.RSI(close_prices, timeperiod=14)[-1]\n            macd, macd_signal, _ = talib.MACD(close_prices)\n            macd_diff = macd[-1] - macd_signal[-1]\n            atr = talib.ATR(high_prices, low_prices, close_prices, timeperiod=14)[-1]\n            adx = talib.ADX(high_prices, low_prices, close_prices, timeperiod=14)[-1]\n            \n            # Volume-based feature (if available)\n            volume_ratio = 1.0\n            if 'Volume' in data.columns:\n                recent_volume = data['Volume'].tail(5).mean()\n                avg_volume = data['Volume'].tail(20).mean()\n                volume_ratio = recent_volume / avg_volume if avg_volume > 0 else 1.0\n            \n            features = [rsi, macd_diff, atr, adx, volume_ratio]\n            return features\n        except Exception as e:\n            logger.warning(f\"Error extracting calibration features: {str(e)}\")\n            return None\n    \n    def _calculate_expected_value(self, probability, market_regime):\n        \"\"\"Calculate Expected Value: EV = (p * avg_win) - ((1-p) * avg_loss)\"\"\"\n        try:\n            # Get historical performance data\n            avg_win = self.ev_tracker.get('avg_win', 20)  # Default 20 pips\n            avg_loss = self.ev_tracker.get('avg_loss', 15)  # Default 15 pips\n            \n            # Adjust for market regime\n            if market_regime.get('regime') == 'TRENDING':\n                avg_win *= 1.2  # Higher wins in trending markets\n            elif market_regime.get('regime') == 'RANGING':\n                avg_win *= 0.8  # Lower wins in ranging markets\n                avg_loss *= 1.1  # Higher losses in ranging markets\n            \n            # Calculate EV\n            ev = (probability * avg_win) - ((1 - probability) * avg_loss)\n            return ev\n        except Exception as e:\n            logger.warning(f\"Error calculating expected value: {str(e)}\")\n            return 0\n    \n    def _get_dynamic_threshold(self, market_regime):\n        \"\"\"Get dynamic confidence threshold based on ADX regime\"\"\"\n        try:\n            adx = market_regime.get('adx', 0)\n            \n            if adx >= 40:\n                return self.confidence_thresholds['strong_trend']['threshold']\n            elif adx >= 25:\n                return self.confidence_thresholds['moderate_trend']['threshold']\n            else:\n                return self.confidence_thresholds['weak_trend']['threshold']\n        except Exception as e:\n            logger.warning(f\"Error getting dynamic threshold: {str(e)}\")\n            return 0.6  # Default threshold\n    \n    def _meta_labeling_decision(self, data, probability, market_regime):\n        \"\"\"Meta-labeling: decide whether to act on the signal\"\"\"\n        try:\n            if self.meta_classifier is None:\n                # If no meta-classifier trained, use simple heuristics\n                if probability > 0.6 and market_regime.get('adx', 0) > 20:\n                    return 1  # Act on signal\n                else:\n                    return 0  # Don't act\n            \n            # Extract meta-features\n            meta_features = self._extract_meta_features(data, probability, market_regime)\n            if meta_features is not None:\n                meta_features_scaled = self.scaler.transform([meta_features])\n                decision = self.meta_classifier.predict(meta_features_scaled)[0]\n                return decision\n            \n            return 0\n        except Exception as e:\n            logger.warning(f\"Error in meta-labeling decision: {str(e)}\")\n            return 0\n    \n    def _extract_meta_features(self, data, probability, market_regime):\n        \"\"\"Extract features for meta-labeling\"\"\"\n        try:\n            # Meta-features: probability, market regime, volatility, time of day\n            adx = market_regime.get('adx', 0)\n            band_width = market_regime.get('band_width', 0)\n            \n            # Time-based feature (hour of day effect)\n            current_hour = datetime.now().hour\n            \n            # Volatility feature\n            if len(data) >= 20:\n                returns = data['Close'].pct_change().tail(20)\n                volatility = returns.std()\n            else:\n                volatility = 0\n            \n            meta_features = [probability, adx, band_width, current_hour, volatility]\n            return meta_features\n        except Exception as e:\n            logger.warning(f\"Error extracting meta-features: {str(e)}\")\n            return None\n    \n    def _calculate_kelly_fraction(self, probability, expected_value):\n        \"\"\"Calculate fractional Kelly sizing: f* = (b*p - (1-p))/b\"\"\"\n        try:\n            # Get win/loss ratio from tracker\n            avg_win = self.ev_tracker.get('avg_win', 20)\n            avg_loss = self.ev_tracker.get('avg_loss', 15)\n            \n            if avg_loss > 0:\n                b = avg_win / avg_loss  # Win/loss ratio\n                kelly_fraction = (b * probability - (1 - probability)) / b\n                \n                # Use fractional Kelly (25% of full Kelly) for safety\n                fractional_kelly = 0.25 * max(0, kelly_fraction)\n                \n                # Cap at 2% for risk management\n                return min(fractional_kelly, 0.02)\n            \n            return 0.01  # Default 1% position size\n        except Exception as e:\n            logger.warning(f\"Error calculating Kelly fraction: {str(e)}\")\n            return 0.01\n    \n    def _determine_institutional_signal(self, weighted_signal, calibrated_probability, \n                                      market_regime, timeframe_consensus, price_action, \n                                      expected_value, dynamic_threshold, meta_decision):\n        \"\"\"Institutional-grade signal determination with all filters\"\"\"\n        try:\n            # Step 1: Check probability against dynamic threshold\n            if calibrated_probability < dynamic_threshold:\n                return 'HOLD', {'percentage': calibrated_probability * 100, 'status': 'BELOW_THRESHOLD'}\n            \n            # Step 2: Check Expected Value filter\n            if expected_value <= 0:\n                return 'HOLD', {'percentage': calibrated_probability * 100, 'status': 'NEGATIVE_EV'}\n            \n            # Step 3: Check meta-labeling decision\n            if meta_decision == 0:\n                return 'HOLD', {'percentage': calibrated_probability * 100, 'status': 'META_REJECT'}\n            \n            # Step 4: Determine signal direction\n            if weighted_signal > 0.1:\n                signal = 'BUY'\n            elif weighted_signal < -0.1:\n                signal = 'SELL'\n            else:\n                signal = 'HOLD'\n            \n            # Step 5: Calculate final confidence\n            base_confidence = calibrated_probability * 100\n            \n            # Boost confidence for strong market regimes\n            if market_regime.get('adx', 0) > 30:\n                base_confidence += 10\n            \n            # Boost for timeframe consensus\n            if timeframe_consensus.get('consensus') == signal:\n                base_confidence += 15\n            \n            # Reduce for price action risks\n            if price_action.get('near_key_level'):\n                base_confidence -= 10\n            \n            # Cap confidence\n            final_confidence = min(100, max(0, base_confidence))\n            \n            # Determine status\n            if final_confidence >= 85:\n                status = 'INSTITUTIONAL_HIGH'\n            elif final_confidence >= 70:\n                status = 'INSTITUTIONAL_MEDIUM'\n            elif final_confidence >= dynamic_threshold * 100:\n                status = 'INSTITUTIONAL_LOW'\n            else:\n                status = 'INSTITUTIONAL_WEAK'\n                signal = 'HOLD'\n            \n            return signal, {\n                'percentage': round(final_confidence, 1),\n                'status': status,\n                'expected_value': expected_value,\n                'dynamic_threshold': dynamic_threshold,\n                'meta_decision': meta_decision,\n                'filters_passed': {\n                    'probability_threshold': calibrated_probability >= dynamic_threshold,\n                    'positive_ev': expected_value > 0,\n                    'meta_approval': meta_decision == 1\n                }\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error in institutional signal determination: {str(e)}\")\n            return 'HOLD', {'percentage': 0, 'status': 'ERROR'}\n\n    def _default_analysis(self):\n        \"\"\"Return default analysis when data is insufficient\"\"\"\n        return {\n            'final_signal': 'HOLD',\n            'confidence': {'percentage': 0, 'status': 'INSUFFICIENT_DATA'},\n            'buy_probability': 0.0,\n            'sell_probability': 0.0,\n            'model_votes': {model: 0 for model in self.models.keys()},\n            'model_details': {},\n            'vote_counts': {'buy': 0, 'sell': 0, 'neutral': 5},\n            'market_regime': {'regime': 'UNKNOWN', 'adx': 0, 'band_width': 0, 'trend_strength': 'Unknown'},\n            'atr_value': {'value': 0, 'sl_multiplier': 1.5, 'tp_multiplier': 3.0},\n            'price_action': {'support': 0, 'resistance': 0, 'near_key_level': False, 'doji_pattern': False, 'price_position': 'Unknown'},\n            'timeframe_consensus': {'consensus': 'NEUTRAL', 'agreement': 0, 'timeframe_signals': {}}\n        }","size_bytes":41211},"main.py":{"content":"from flask import Flask, render_template, request, jsonify\nimport os\nfrom datetime import datetime\nimport traceback\nimport logging\nimport numpy as np\nimport pandas as pd\n\n# Configure logging\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\ndef sanitize_for_json(obj):\n    \"\"\"Convert NumPy/Pandas objects to JSON-serializable types\"\"\"\n    if isinstance(obj, dict):\n        return {key: sanitize_for_json(value) for key, value in obj.items()}\n    elif isinstance(obj, list):\n        return [sanitize_for_json(item) for item in obj]\n    elif isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    elif isinstance(obj, (pd.Timestamp, pd.Timedelta)):\n        return str(obj)\n    elif hasattr(obj, 'item'):  # NumPy scalar\n        return obj.item()\n    elif pd.isna(obj):\n        return None\n    else:\n        return obj\nfrom trading.data_fetcher import ForexDataFetcher\nfrom trading.ensemble_analyzer import EnsembleAnalyzer\nfrom trading.risk_manager import RiskManager\nfrom trading.performance_tracker import PerformanceTracker\nfrom trading.backtester import InstitutionalBacktester\n\napp = Flask(__name__)\napp.secret_key = os.environ.get('SESSION_SECRET')\n\n# Initialize trading components\ndata_fetcher = ForexDataFetcher()\nensemble_analyzer = EnsembleAnalyzer()\nrisk_manager = RiskManager()\nperformance_tracker = PerformanceTracker()\nbacktester = InstitutionalBacktester(ensemble_analyzer, risk_manager)\n\n@app.route('/')\ndef index():\n    \"\"\"Main dashboard page\"\"\"\n    return render_template('dashboard.html')\n\n@app.route('/api/signals')\ndef get_signals():\n    \"\"\"Enhanced API endpoint with multi-timeframe analysis and ATR-based calculations\"\"\"\n    try:\n        account_size = request.args.get('account_size', 10000, type=float)\n        risk_percent = request.args.get('risk_percent', 1.5, type=float)\n        \n        # Currency pairs to analyze\n        pairs = ['EURUSD=X', 'USDJPY=X']\n        results = {}\n        \n        for pair in pairs:\n            # Fetch intraday data optimized for 5m-15m institutional analysis\n            data_15m = data_fetcher.get_forex_data(pair, period='30d', interval='15m')\n            multi_timeframe_data = data_fetcher.get_multi_timeframe_data(pair)\n            \n            if data_15m is not None and not data_15m.empty:\n                # Enhanced ensemble analysis with intraday 15m data\n                analysis = ensemble_analyzer.analyze(data_15m, multi_timeframe_data)\n                \n                # Calculate institutional-grade risk management levels\n                current_price = data_15m['Close'].iloc[-1]\n                \n                # Extract Kelly fraction and institutional features from analysis (with safe defaults)\n                kelly_fraction = analysis.get('kelly_fraction', None)\n                timeframe = '15m'  # Default intraday timeframe\n                \n                # Calculate risk levels with error handling for institutional features\n                try:\n                    risk_calculations = risk_manager.calculate_risk_levels(\n                        pair=pair.replace('=X', ''),\n                        current_price=current_price,\n                        account_size=account_size,\n                        risk_percent=risk_percent,\n                        atr_data=analysis.get('atr_value'),\n                        timeframe=timeframe,\n                        kelly_fraction=kelly_fraction\n                    )\n                except Exception as risk_error:\n                    logger.warning(f\"Institutional risk calculation failed, using basic method: {str(risk_error)}\")\n                    # Fallback to basic risk calculation\n                    risk_calculations = risk_manager.calculate_risk_levels(\n                        pair=pair.replace('=X', ''),\n                        current_price=current_price,\n                        account_size=account_size,\n                        risk_percent=risk_percent,\n                        atr_data=analysis.get('atr_value')\n                    )\n                \n                # Record signal for performance tracking\n                if analysis['final_signal'] in ['BUY', 'SELL']:\n                    performance_tracker.record_signal(\n                        pair=pair.replace('=X', ''),\n                        signal=analysis['final_signal'],\n                        confidence=analysis['confidence'],\n                        entry_price=current_price,\n                        sl_price=risk_calculations['stop_loss']['buy' if analysis['final_signal'] == 'BUY' else 'sell'],\n                        tp_price=risk_calculations['take_profit']['buy' if analysis['final_signal'] == 'BUY' else 'sell']\n                    )\n                \n                # Combine analysis and risk data with enhanced information\n                pair_name = pair.replace('=X', '').replace('USD', '/USD')\n                results[pair_name] = {\n                    'timestamp': datetime.now().isoformat(),\n                    'current_price': round(current_price, 5),\n                    'signal': analysis['final_signal'],\n                    'confidence': analysis['confidence'],\n                    'buy_probability': analysis['buy_probability'],\n                    'sell_probability': analysis['sell_probability'],\n                    'model_votes': analysis['model_votes'],\n                    'market_regime': analysis['market_regime'],\n                    'atr_value': analysis['atr_value'],\n                    'price_action': analysis['price_action'],\n                    'timeframe_consensus': analysis['timeframe_consensus'],\n                    'stop_loss': risk_calculations['stop_loss'],\n                    'take_profit': risk_calculations['take_profit'],\n                    'position_size': risk_calculations['position_size'],\n                    'risk_metrics': risk_calculations.get('risk_metrics', {'risk_amount': 0, 'risk_percent': 0}),\n                    'session_limits': risk_calculations.get('session_limits', {}),\n                    'reward_risk_ratio': risk_calculations.get('reward_risk_ratio', 2.0),\n                    'timeframe': risk_calculations.get('timeframe', '15m'),\n                    'calculation_method': risk_calculations.get('calculation_method', 'Basic'),\n                    'institutional_features': risk_calculations.get('institutional_features', {}),\n                    # Legacy compatibility\n                    'risk_amount': risk_calculations.get('risk_metrics', {}).get('risk_amount', \n                                                       risk_calculations.get('risk_amount', 0))\n                }\n            else:\n                pair_name = pair.replace('=X', '').replace('USD', '/USD')\n                results[pair_name] = {\n                    'error': 'Unable to fetch data for this pair'\n                }\n        \n        # Get performance statistics\n        performance_summary = performance_tracker.get_performance_summary()\n        \n        # Sanitize results for JSON serialization\n        response_data = {\n            'success': True,\n            'data': results,\n            'performance': performance_summary,\n            'timestamp': datetime.now().isoformat()\n        }\n        return jsonify(sanitize_for_json(response_data))\n        \n    except Exception as e:\n        return jsonify({\n            'success': False,\n            'error': str(e),\n            'traceback': traceback.format_exc()\n        }), 500\n\n@app.route('/api/signal')\ndef get_institutional_signal():\n    \"\"\"Enhanced institutional signal endpoint with EV, Kelly sizing, and comprehensive risk metrics\"\"\"\n    try:\n        account_size = request.args.get('account_size', 10000, type=float)\n        risk_percent = request.args.get('risk_percent', 0.5, type=float)  # Conservative for intraday\n        pair = request.args.get('pair', 'EURUSD', type=str)\n        timeframe = request.args.get('timeframe', '15m', type=str)\n        \n        # Fetch intraday data\n        if timeframe == '5m':\n            data = data_fetcher.get_forex_data(f'{pair}=X', period='10d', interval='5m')\n        elif timeframe == '15m':\n            data = data_fetcher.get_forex_data(f'{pair}=X', period='30d', interval='15m')\n        else:\n            data = data_fetcher.get_forex_data(f'{pair}=X', period='60d', interval='1h')\n        \n        if data is None or data.empty:\n            return jsonify({\n                'success': False,\n                'error': f'Unable to fetch data for {pair}',\n                'timestamp': datetime.now().isoformat()\n            }), 400\n        \n        # Get institutional analysis\n        analysis = ensemble_analyzer.analyze(data)\n        current_price = data['Close'].iloc[-1]\n        \n        # Calculate institutional risk levels\n        risk_calculations = risk_manager.calculate_risk_levels(\n            pair=pair,\n            current_price=current_price,\n            account_size=account_size,\n            risk_percent=risk_percent,\n            atr_data=analysis.get('atr_value'),\n            timeframe=timeframe,\n            kelly_fraction=analysis.get('kelly_fraction')\n        )\n        \n        # Get risk summary\n        risk_summary = risk_manager.get_risk_summary(account_size)\n        \n        # Prepare institutional response data\n        institutional_response = {\n            'success': True,\n            'signal_data': {\n                'pair': pair,\n                'timeframe': timeframe,\n                'timestamp': datetime.now().isoformat(),\n                'current_price': round(current_price, 5),\n                'signal': analysis['final_signal'],\n                'confidence': analysis['confidence'],\n                'institutional_metrics': {\n                    'weighted_signal': analysis.get('weighted_signal', 0),\n                    'calibrated_probability': analysis.get('calibrated_probability', 0.5),\n                    'expected_value': analysis.get('expected_value', 0),\n                    'kelly_fraction': analysis.get('kelly_fraction', 0),\n                    'dynamic_threshold': analysis.get('dynamic_threshold', 0.6),\n                    'meta_decision': analysis.get('meta_decision', 1)\n                },\n                'model_performance': {\n                    'model_votes': analysis['model_votes'],\n                    'model_weights': analysis.get('model_weights', {}),\n                    'ensemble_details': analysis.get('model_details', {})\n                },\n                'market_analysis': {\n                    'regime': analysis['market_regime'],\n                    'price_action': analysis['price_action'],\n                    'timeframe_consensus': analysis['timeframe_consensus']\n                },\n                'risk_management': risk_calculations,\n                'session_summary': risk_summary\n            },\n            'filters_status': {\n                'probability_filter': analysis.get('calibrated_probability', 0.5) >= analysis.get('dynamic_threshold', 0.6),\n                'ev_filter': analysis.get('expected_value', 0) > 0,\n                'meta_filter': analysis.get('meta_decision', 1) == 1,\n                'all_passed': (analysis.get('calibrated_probability', 0.5) >= analysis.get('dynamic_threshold', 0.6) and \n                              analysis.get('expected_value', 0) > 0 and \n                              analysis.get('meta_decision', 1) == 1)\n            }\n        }\n        \n        # Sanitize and return response\n        return jsonify(sanitize_for_json(institutional_response))\n        \n    except Exception as e:\n        return jsonify({\n            'success': False,\n            'error': str(e),\n            'traceback': traceback.format_exc()\n        }), 500\n\n@app.route('/api/backtest')\ndef run_backtest():\n    \"\"\"Run institutional-grade backtest with comprehensive analytics\"\"\"\n    try:\n        pair = request.args.get('pair', 'EURUSD', type=str)\n        timeframe = request.args.get('timeframe', '15m', type=str)\n        account_size = request.args.get('account_size', 10000, type=float)\n        \n        # Fetch sufficient data for backtesting\n        period_map = {'5m': '90d', '15m': '180d', '1h': '365d'}\n        interval_map = {'5m': '5m', '15m': '15m', '1h': '1h'}\n        \n        data = data_fetcher.get_forex_data(\n            f'{pair}=X', \n            period=period_map.get(timeframe, '180d'), \n            interval=interval_map.get(timeframe, '15m')\n        )\n        \n        if data is None or data.empty or len(data) < 1000:\n            return jsonify({\n                'success': False,\n                'error': f'Insufficient data for backtesting {pair} on {timeframe}',\n                'data_points': len(data) if data is not None else 0\n            }), 400\n        \n        # Run comprehensive backtest\n        backtest_results = backtester.run_walk_forward_backtest(\n            data=data,\n            pair=pair,\n            timeframe=timeframe,\n            account_size=account_size\n        )\n        \n        # Get backtest summary for comparison\n        backtest_summary = backtester.get_backtest_summary(pair, timeframe)\n        \n        return jsonify({\n            'success': True,\n            'backtest_results': backtest_results,\n            'historical_summary': backtest_summary,\n            'timestamp': datetime.now().isoformat()\n        })\n        \n    except Exception as e:\n        return jsonify({\n            'success': False,\n            'error': str(e),\n            'traceback': traceback.format_exc()\n        }), 500\n\n@app.route('/backtest')\ndef backtest_dashboard():\n    \"\"\"Institutional backtest analytics dashboard\"\"\"\n    return render_template('backtest_dashboard.html')\n\n@app.route('/api/health')\ndef health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return jsonify({\n        'status': 'healthy',\n        'timestamp': datetime.now().isoformat(),\n        'version': '2.0.0',  # Updated version for institutional features\n        'institutional_features': {\n            'sharpe_weighted_ensemble': True,\n            'probability_calibration': True,\n            'dynamic_thresholds': True,\n            'expected_value_filter': True,\n            'kelly_sizing': True,\n            'meta_labeling': True,\n            'var_cvar_monitoring': True,\n            'walk_forward_backtesting': True,\n            'triple_barrier_labeling': True\n        }\n    })\n\nif __name__ == '__main__':\n    # Run with host 0.0.0.0 to allow external connections\n    app.run(host='0.0.0.0', port=5000, debug=True)","size_bytes":14590},"trading/data_fetcher.py":{"content":"import yfinance as yf\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass ForexDataFetcher:\n    \"\"\"Fetches real-time forex data using yfinance\"\"\"\n    \n    def __init__(self):\n        self.cache = {}\n        self.cache_duration = 300  # 5 minutes cache\n    \n    def get_forex_data(self, pair, period='1d', interval='1h'):\n        \"\"\"\n        Fetch forex data for a given pair\n        \n        Args:\n            pair (str): Currency pair (e.g., 'EURUSD=X', 'USDJPY=X')\n            period (str): Data period ('1d', '5d', '1mo', etc.)\n            interval (str): Data interval ('1m', '5m', '15m', '1h', etc.)\n        \n        Returns:\n            pandas.DataFrame: OHLCV data with technical indicators\n        \"\"\"\n        try:\n            # Check cache first\n            cache_key = f\"{pair}_{period}_{interval}\"\n            current_time = datetime.now()\n            \n            if cache_key in self.cache:\n                cached_data, cache_time = self.cache[cache_key]\n                if (current_time - cache_time).seconds < self.cache_duration:\n                    logger.info(f\"Using cached data for {pair}\")\n                    return cached_data\n            \n            # Fetch fresh data\n            logger.info(f\"Fetching fresh data for {pair}\")\n            ticker = yf.Ticker(pair)\n            data = ticker.history(period=period, interval=interval)\n            \n            if data.empty:\n                logger.warning(f\"No data returned for {pair}\")\n                return None\n            \n            # Clean and prepare data\n            data = data.dropna()\n            \n            # Add basic price data if needed\n            if 'Adj Close' not in data.columns:\n                data['Adj Close'] = data['Close']\n            \n            # Cache the data\n            self.cache[cache_key] = (data, current_time)\n            \n            logger.info(f\"Successfully fetched {len(data)} rows for {pair}\")\n            return data\n            \n        except Exception as e:\n            logger.error(f\"Error fetching data for {pair}: {str(e)}\")\n            return None\n    \n    def get_multi_timeframe_data(self, pair):\n        \"\"\"\n        Fetch multi-timeframe data for comprehensive analysis\n        \n        Args:\n            pair (str): Currency pair (e.g., 'EURUSD=X', 'USDJPY=X')\n        \n        Returns:\n            dict: Data for different timeframes\n        \"\"\"\n        try:\n            # Intraday-focused timeframes for institutional analysis (5m-15m optimized)\n            timeframes = {\n                '5M': ('7d', '5m'),     # 5-minute data for 7 days (high frequency)\n                '15M': ('30d', '15m'),  # 15-minute data for 30 days (primary timeframe)\n                '1H': ('60d', '1h'),    # 1-hour data for trend confirmation\n            }\n            \n            multi_data = {}\n            \n            for tf_name, (period, interval) in timeframes.items():\n                try:\n                    data = self.get_forex_data(pair, period=period, interval=interval)\n                    if data is not None and not data.empty:\n                        multi_data[tf_name] = data\n                    else:\n                        multi_data[tf_name] = None\n                        logger.warning(f\"No {tf_name} data for {pair}\")\n                except Exception as e:\n                    logger.warning(f\"Error fetching {tf_name} data for {pair}: {str(e)}\")\n                    multi_data[tf_name] = None\n            \n            return multi_data\n            \n        except Exception as e:\n            logger.error(f\"Error fetching multi-timeframe data for {pair}: {str(e)}\")\n            return {}\n    \n    def get_current_price(self, pair):\n        \"\"\"Get current price for a currency pair\"\"\"\n        try:\n            data = self.get_forex_data(pair, period='1d', interval='1m')\n            if data is not None and not data.empty:\n                return data['Close'].iloc[-1]\n            return None\n        except Exception as e:\n            logger.error(f\"Error getting current price for {pair}: {str(e)}\")\n            return None","size_bytes":4198},"trading/risk_manager.py":{"content":"import logging\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom scipy import stats\nimport json\nimport os\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass RiskManager:\n    \"\"\"Institutional-Grade Risk Management with VaR/CVaR, Kelly Sizing, and Intraday Optimization\"\"\"\n    \n    def __init__(self):\n        # Pip values for different currency pairs (in USD for 1 standard lot)\n        self.pip_values = {\n            'EURUSD': 10.0,  # $10 per pip for 1 standard lot\n            'USDJPY': 10.0,  # Approximately $10 per pip (varies with JPY rate)\n            'GBPUSD': 10.0,\n            'AUDUSD': 10.0,\n            'USDCAD': 10.0,\n            'USDCHF': 10.0\n        }\n        \n        # Intraday stop loss pips for 5m-15m timeframes (scaled down)\n        self.intraday_stop_loss_pips = {\n            'EURUSD': {'5m': 5, '15m': 8, '1h': 15},\n            'USDJPY': {'5m': 7, '15m': 12, '1h': 20},\n            'GBPUSD': {'5m': 6, '15m': 10, '1h': 18},\n            'AUDUSD': {'5m': 5, '15m': 9, '1h': 16},\n            'USDCAD': {'5m': 5, '15m': 9, '1h': 16},\n            'USDCHF': {'5m': 5, '15m': 8, '1h': 15}\n        }\n        \n        # Intraday take profit pips (2:1 ratios maintained)\n        self.intraday_take_profit_pips = {\n            'EURUSD': {'5m': 10, '15m': 16, '1h': 30},\n            'USDJPY': {'5m': 14, '15m': 24, '1h': 40},\n            'GBPUSD': {'5m': 12, '15m': 20, '1h': 36},\n            'AUDUSD': {'5m': 10, '15m': 18, '1h': 32},\n            'USDCAD': {'5m': 10, '15m': 18, '1h': 32},\n            'USDCHF': {'5m': 10, '15m': 16, '1h': 30}\n        }\n        \n        # Risk budgets for intraday trading\n        self.intraday_risk_limits = {\n            'per_trade': 0.5,  # 0.5% per trade (reduced for higher frequency)\n            'per_session': 2.0,  # 2% per trading session\n            'daily_max': 3.0     # 3% daily maximum\n        }\n        \n        # VaR/CVaR parameters\n        self.var_confidence = 0.95  # 95% confidence level\n        self.var_lookback = 100     # 100-period lookback\n        \n        # Performance tracking for risk metrics\n        self.performance_tracker = self._load_performance_data()\n        \n        # Session risk tracking\n        self.session_risk = {\n            'current_exposure': 0.0,\n            'trades_today': 0,\n            'session_pnl': 0.0,\n            'max_drawdown': 0.0,\n            'peak_equity': 0.0\n        }\n    \n    def calculate_risk_levels(self, pair, current_price, account_size=10000.0, risk_percent=1.5, atr_data=None, \n                            timeframe='15m', kelly_fraction=None, volatility_cap=None):\n        \"\"\"\n        Institutional-grade risk calculation with Kelly sizing, VaR limits, and intraday optimization\n        \n        Args:\n            pair (str): Currency pair (e.g., 'EURUSD')\n            current_price (float): Current market price\n            account_size (float): Account balance in USD\n            risk_percent (float): Base risk percentage per trade\n            atr_data (dict): ATR data from ensemble analysis (optional)\n            timeframe (str): Trading timeframe ('5m', '15m', '1h')\n            kelly_fraction (float): Kelly fraction from ensemble analyzer\n            volatility_cap (float): Volatility-based position size cap\n        \n        Returns:\n            dict: Comprehensive institutional risk management data\n        \"\"\"\n        try:\n            # Calculate pip size based on currency pair\n            if 'JPY' in pair:\n                pip_size = 0.01  # For JPY pairs, 1 pip = 0.01\n            else:\n                pip_size = 0.0001  # For other pairs, 1 pip = 0.0001\n            \n            # Get intraday-optimized pip values\n            timeframe = timeframe if timeframe in ['5m', '15m', '1h'] else '15m'\n            \n            # Use ATR-based calculations if available, otherwise use intraday pips\n            if atr_data and atr_data.get('value', 0) > 0:\n                # ATR-based dynamic levels (scaled for intraday)\n                atr_value = atr_data['value']\n                intraday_multiplier = {'5m': 1.0, '15m': 1.2, '1h': 1.5}[timeframe]\n                \n                sl_distance = atr_value * 1.5 * intraday_multiplier\n                tp_distance = atr_value * 3.0 * intraday_multiplier\n                \n                sl_pips = sl_distance / pip_size\n                tp_pips = tp_distance / pip_size\n                \n                calculation_method = f'ATR-based ({timeframe})'\n            else:\n                # Use intraday-optimized fixed pips\n                sl_pips = self.intraday_stop_loss_pips.get(pair, {}).get(timeframe, 8)\n                tp_pips = self.intraday_take_profit_pips.get(pair, {}).get(timeframe, 16)\n                sl_distance = sl_pips * pip_size\n                tp_distance = tp_pips * pip_size\n                \n                calculation_method = f'Intraday-optimized ({timeframe})'\n            \n            # Get pip value for position sizing\n            pip_value = self.pip_values.get(pair, 10.0)\n            \n            # Institutional position sizing with Kelly fraction and risk limits\n            base_risk_percent = min(risk_percent, self.intraday_risk_limits['per_trade'])\n            \n            # Apply Kelly fraction if provided\n            if kelly_fraction and kelly_fraction > 0:\n                adjusted_risk_percent = min(base_risk_percent, kelly_fraction * 100)\n            else:\n                adjusted_risk_percent = base_risk_percent\n            \n            # Check session risk limits\n            if self._check_session_risk_limits(account_size, adjusted_risk_percent):\n                adjusted_risk_percent *= 0.5  # Reduce position size if approaching limits\n            \n            risk_amount = account_size * (adjusted_risk_percent / 100)\n            \n            # Calculate base position size\n            position_size_lots = risk_amount / (sl_pips * pip_value)\n            \n            # Apply volatility cap if provided\n            if volatility_cap and volatility_cap > 0:\n                volatility_adjusted_size = min(position_size_lots, volatility_cap)\n                position_size_lots = volatility_adjusted_size\n            \n            # Convert to micro lots for practical trading\n            micro_lots = position_size_lots * 100  # 1 standard lot = 100 micro lots\n            micro_lots = max(0.01, round(micro_lots, 2))  # Minimum 0.01 micro lots\n            \n            # Calculate actual risk with position size\n            actual_risk = (micro_lots / 100) * sl_pips * pip_value\n            \n            # Calculate VaR and CVaR\n            var_metrics = self._calculate_var_cvar(actual_risk, pair)\n            \n            # Calculate Calmar ratio and max drawdown\n            performance_metrics = self._calculate_performance_metrics()\n            \n            # Calculate stop loss and take profit prices using actual distances\n            stop_loss_buy = round(current_price - sl_distance, 5)\n            stop_loss_sell = round(current_price + sl_distance, 5)\n            take_profit_buy = round(current_price + tp_distance, 5)\n            take_profit_sell = round(current_price - tp_distance, 5)\n            \n            return {\n                'stop_loss': {\n                    'buy': stop_loss_buy,\n                    'sell': stop_loss_sell,\n                    'pips': round(sl_pips, 1),\n                    'distance': round(sl_distance, 5)\n                },\n                'take_profit': {\n                    'buy': take_profit_buy,\n                    'sell': take_profit_sell,\n                    'pips': round(tp_pips, 1),\n                    'distance': round(tp_distance, 5)\n                },\n                'position_size': {\n                    'micro_lots': micro_lots,\n                    'standard_lots': round(position_size_lots, 4),\n                    'units': int(micro_lots * 1000),\n                    'kelly_adjusted': kelly_fraction is not None,\n                    'volatility_capped': volatility_cap is not None\n                },\n                'risk_metrics': {\n                    'risk_amount': round(actual_risk, 2),\n                    'risk_percent': round((actual_risk / account_size) * 100, 2),\n                    'base_risk_percent': base_risk_percent,\n                    'adjusted_risk_percent': adjusted_risk_percent,\n                    'var_95': var_metrics['var_95'],\n                    'cvar_95': var_metrics['cvar_95'],\n                    'max_drawdown': performance_metrics['max_drawdown'],\n                    'calmar_ratio': performance_metrics['calmar_ratio']\n                },\n                'session_limits': {\n                    'current_exposure': self.session_risk['current_exposure'],\n                    'trades_today': self.session_risk['trades_today'],\n                    'session_pnl': self.session_risk['session_pnl'],\n                    'daily_limit_used': (self.session_risk['current_exposure'] / account_size) * 100\n                },\n                'reward_risk_ratio': round(tp_pips / sl_pips, 2) if sl_pips > 0 else 2.0,\n                'pip_value': pip_value,\n                'pip_size': pip_size,\n                'timeframe': timeframe,\n                'calculation_method': calculation_method,\n                'institutional_features': {\n                    'atr_used': atr_data is not None and atr_data.get('value', 0) > 0,\n                    'kelly_sizing': kelly_fraction is not None,\n                    'volatility_cap': volatility_cap is not None,\n                    'session_limits': True,\n                    'var_monitoring': True\n                }\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error calculating risk levels for {pair}: {str(e)}\")\n            return self._default_risk_calculation(current_price)\n    \n    def _load_performance_data(self):\n        \"\"\"Load historical performance data for risk calculations\"\"\"\n        try:\n            if os.path.exists('risk_performance.json'):\n                with open('risk_performance.json', 'r') as f:\n                    return json.load(f)\n        except Exception as e:\n            logger.warning(f\"Error loading performance data: {str(e)}\")\n        \n        # Initialize performance data\n        return {\n            'daily_returns': [],\n            'trade_pnl': [],\n            'drawdowns': [],\n            'peak_equity': 0.0,\n            'max_drawdown': 0.0,\n            'total_return': 0.0,\n            'volatility': 0.0,\n            'sharpe_ratio': 0.0,\n            'calmar_ratio': 0.0\n        }\n    \n    def _check_session_risk_limits(self, account_size, risk_percent):\n        \"\"\"Check if trade would breach session risk limits\"\"\"\n        try:\n            # Calculate potential new exposure\n            potential_risk = account_size * (risk_percent / 100)\n            new_exposure = self.session_risk['current_exposure'] + potential_risk\n            \n            # Check various limits\n            session_limit = account_size * (self.intraday_risk_limits['per_session'] / 100)\n            daily_limit = account_size * (self.intraday_risk_limits['daily_max'] / 100)\n            \n            # Return True if approaching limits (>80% of limit)\n            return (new_exposure > session_limit * 0.8) or (new_exposure > daily_limit * 0.8)\n        except Exception as e:\n            logger.warning(f\"Error checking session risk limits: {str(e)}\")\n            return False\n    \n    def _calculate_var_cvar(self, position_risk, pair):\n        \"\"\"Calculate Value at Risk and Conditional Value at Risk\"\"\"\n        try:\n            # Get historical returns for the pair\n            returns = self.performance_tracker.get('daily_returns', [])\n            \n            if len(returns) < 30:  # Need minimum data\n                return {\n                    'var_95': position_risk * 2.0,  # Conservative estimate\n                    'cvar_95': position_risk * 3.0,\n                    'var_99': position_risk * 2.5,\n                    'data_points': len(returns)\n                }\n            \n            # Convert to numpy array and calculate percentiles\n            returns_array = np.array(returns[-self.var_lookback:])\n            \n            # Calculate VaR at different confidence levels\n            var_95 = np.percentile(returns_array, 5)  # 5th percentile for 95% VaR\n            var_99 = np.percentile(returns_array, 1)  # 1st percentile for 99% VaR\n            \n            # Calculate CVaR (Expected Shortfall)\n            # Average of returns below VaR threshold\n            tail_returns_95 = returns_array[returns_array <= var_95]\n            cvar_95 = np.mean(tail_returns_95) if len(tail_returns_95) > 0 else var_95\n            \n            tail_returns_99 = returns_array[returns_array <= var_99]\n            cvar_99 = np.mean(tail_returns_99) if len(tail_returns_99) > 0 else var_99\n            \n            # Scale by position size\n            return {\n                'var_95': abs(var_95 * position_risk),\n                'var_99': abs(var_99 * position_risk),\n                'cvar_95': abs(cvar_95 * position_risk),\n                'cvar_99': abs(cvar_99 * position_risk),\n                'data_points': len(returns_array)\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error calculating VaR/CVaR: {str(e)}\")\n            return {\n                'var_95': position_risk * 2.0,\n                'cvar_95': position_risk * 3.0,\n                'var_99': position_risk * 2.5,\n                'cvar_99': position_risk * 3.5,\n                'data_points': 0\n            }\n    \n    def _calculate_performance_metrics(self):\n        \"\"\"Calculate rolling performance metrics including Calmar ratio\"\"\"\n        try:\n            returns = self.performance_tracker.get('daily_returns', [])\n            \n            if len(returns) < 10:\n                return {\n                    'max_drawdown': 0.0,\n                    'calmar_ratio': 0.0,\n                    'sharpe_ratio': 0.0,\n                    'volatility': 0.0,\n                    'total_return': 0.0\n                }\n            \n            returns_array = np.array(returns)\n            \n            # Calculate cumulative returns\n            cumulative_returns = np.cumprod(1 + returns_array)\n            \n            # Calculate running maximum (peak equity)\n            running_max = np.maximum.accumulate(cumulative_returns)\n            \n            # Calculate drawdowns\n            drawdowns = (cumulative_returns - running_max) / running_max\n            max_drawdown = abs(np.min(drawdowns)) if len(drawdowns) > 0 else 0.0\n            \n            # Update session tracking\n            self.session_risk['max_drawdown'] = max_drawdown\n            self.session_risk['peak_equity'] = running_max[-1] if len(running_max) > 0 else 0.0\n            \n            # Calculate annualized metrics\n            mean_return = np.mean(returns_array)\n            std_return = np.std(returns_array)\n            total_return = cumulative_returns[-1] - 1 if len(cumulative_returns) > 0 else 0.0\n            \n            # Annualized Sharpe ratio (assuming 252 trading days)\n            sharpe_ratio = (mean_return * 252) / (std_return * np.sqrt(252)) if std_return > 0 else 0.0\n            \n            # Calmar ratio (annualized return / max drawdown)\n            calmar_ratio = (total_return * 252 / len(returns)) / max_drawdown if max_drawdown > 0 else 0.0\n            \n            return {\n                'max_drawdown': round(max_drawdown, 4),\n                'calmar_ratio': round(calmar_ratio, 2),\n                'sharpe_ratio': round(sharpe_ratio, 2),\n                'volatility': round(std_return * np.sqrt(252), 4),\n                'total_return': round(total_return, 4),\n                'current_drawdown': round(drawdowns[-1], 4) if len(drawdowns) > 0 else 0.0\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error calculating performance metrics: {str(e)}\")\n            return {\n                'max_drawdown': 0.0,\n                'calmar_ratio': 0.0,\n                'sharpe_ratio': 0.0,\n                'volatility': 0.0,\n                'total_return': 0.0,\n                'current_drawdown': 0.0\n            }\n    \n    def update_session_risk(self, trade_pnl, account_size):\n        \"\"\"Update session risk tracking with new trade\"\"\"\n        try:\n            self.session_risk['trades_today'] += 1\n            self.session_risk['session_pnl'] += trade_pnl\n            \n            # Update performance tracking\n            daily_return = trade_pnl / account_size\n            self.performance_tracker['daily_returns'].append(daily_return)\n            self.performance_tracker['trade_pnl'].append(trade_pnl)\n            \n            # Keep only last 252 days (1 year)\n            if len(self.performance_tracker['daily_returns']) > 252:\n                self.performance_tracker['daily_returns'].pop(0)\n                self.performance_tracker['trade_pnl'].pop(0)\n            \n            # Save performance data\n            try:\n                with open('risk_performance.json', 'w') as f:\n                    json.dump(self.performance_tracker, f, indent=2)\n            except Exception:\n                pass\n                \n        except Exception as e:\n            logger.warning(f\"Error updating session risk: {str(e)}\")\n    \n    def get_risk_summary(self, account_size):\n        \"\"\"Get comprehensive risk summary for dashboard\"\"\"\n        try:\n            performance_metrics = self._calculate_performance_metrics()\n            \n            return {\n                'session_metrics': {\n                    'trades_today': self.session_risk['trades_today'],\n                    'session_pnl': round(self.session_risk['session_pnl'], 2),\n                    'current_exposure': round(self.session_risk['current_exposure'], 2),\n                    'exposure_percent': round((self.session_risk['current_exposure'] / account_size) * 100, 2)\n                },\n                'risk_limits': {\n                    'per_trade_limit': self.intraday_risk_limits['per_trade'],\n                    'session_limit': self.intraday_risk_limits['per_session'],\n                    'daily_limit': self.intraday_risk_limits['daily_max'],\n                    'session_limit_used': round((self.session_risk['current_exposure'] / (account_size * self.intraday_risk_limits['per_session'] / 100)) * 100, 1)\n                },\n                'performance_metrics': performance_metrics,\n                'var_summary': {\n                    'confidence_level': self.var_confidence,\n                    'lookback_periods': self.var_lookback,\n                    'data_points': len(self.performance_tracker.get('daily_returns', []))\n                }\n            }\n        except Exception as e:\n            logger.warning(f\"Error getting risk summary: {str(e)}\")\n            return {}\n    \n    def _default_risk_calculation(self, current_price):\n        \"\"\"Return default risk calculation when error occurs\"\"\"\n        return {\n            'stop_loss': {'buy': current_price * 0.998, 'sell': current_price * 1.002, 'pips': 8},\n            'take_profit': {'buy': current_price * 1.004, 'sell': current_price * 0.996, 'pips': 16},\n            'position_size': {'micro_lots': 0.01, 'standard_lots': 0.0001, 'units': 10},\n            'risk_metrics': {\n                'risk_amount': 0,\n                'risk_percent': 0,\n                'var_95': 0,\n                'cvar_95': 0,\n                'max_drawdown': 0,\n                'calmar_ratio': 0\n            },\n            'session_limits': {\n                'current_exposure': 0,\n                'trades_today': 0,\n                'session_pnl': 0,\n                'daily_limit_used': 0\n            },\n            'reward_risk_ratio': 2.0,\n            'pip_value': 10.0,\n            'pip_size': 0.0001,\n            'timeframe': '15m',\n            'calculation_method': 'Default (Error)',\n            'institutional_features': {\n                'atr_used': False,\n                'kelly_sizing': False,\n                'volatility_cap': False,\n                'session_limits': True,\n                'var_monitoring': True\n            }\n        }","size_bytes":20246},"trading/__init__.py":{"content":"# Trading modules for Forex analysis system","size_bytes":43},"trading/performance_tracker.py":{"content":"import json\nimport os\nfrom datetime import datetime, timedelta\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass PerformanceTracker:\n    \"\"\"Performance tracking system for trading strategy\"\"\"\n    \n    def __init__(self, data_file='trading_performance.json'):\n        self.data_file = data_file\n        self.performance_data = self._load_performance_data()\n    \n    def _load_performance_data(self):\n        \"\"\"Load performance data from file\"\"\"\n        try:\n            if os.path.exists(self.data_file):\n                with open(self.data_file, 'r') as f:\n                    data = json.load(f)\n                    logger.info(f\"Loaded performance data: {len(data.get('trades', []))} trades\")\n                    return data\n            else:\n                return self._initialize_performance_data()\n        except Exception as e:\n            logger.warning(f\"Error loading performance data: {str(e)}\")\n            return self._initialize_performance_data()\n    \n    def _initialize_performance_data(self):\n        \"\"\"Initialize empty performance data structure\"\"\"\n        return {\n            'trades': [],\n            'statistics': {\n                'total_trades': 0,\n                'winning_trades': 0,\n                'losing_trades': 0,\n                'win_rate': 0.0,\n                'profit_factor': 0.0,\n                'total_profit': 0.0,\n                'total_loss': 0.0,\n                'largest_win': 0.0,\n                'largest_loss': 0.0,\n                'current_streak': 0,\n                'best_streak': 0,\n                'worst_streak': 0,\n                'avg_win': 0.0,\n                'avg_loss': 0.0\n            },\n            'last_updated': datetime.now().isoformat(),\n            'system_start_date': datetime.now().isoformat()\n        }\n    \n    def _save_performance_data(self):\n        \"\"\"Save performance data to file\"\"\"\n        try:\n            self.performance_data['last_updated'] = datetime.now().isoformat()\n            with open(self.data_file, 'w') as f:\n                json.dump(self.performance_data, f, indent=2)\n            logger.info(\"Performance data saved successfully\")\n        except Exception as e:\n            logger.error(f\"Error saving performance data: {str(e)}\")\n    \n    def record_signal(self, pair, signal, confidence, entry_price, sl_price, tp_price):\n        \"\"\"\n        Record a trading signal (for simulation/tracking purposes)\n        \n        Args:\n            pair (str): Currency pair\n            signal (str): BUY/SELL/HOLD\n            confidence (dict): Confidence data\n            entry_price (float): Entry price\n            sl_price (float): Stop loss price\n            tp_price (float): Take profit price\n        \"\"\"\n        try:\n            if signal in ['BUY', 'SELL']:\n                trade = {\n                    'timestamp': datetime.now().isoformat(),\n                    'pair': pair,\n                    'signal': signal,\n                    'confidence': confidence,\n                    'entry_price': entry_price,\n                    'sl_price': sl_price,\n                    'tp_price': tp_price,\n                    'status': 'OPEN',\n                    'exit_price': None,\n                    'profit_loss': None,\n                    'result': None\n                }\n                \n                self.performance_data['trades'].append(trade)\n                logger.info(f\"Recorded {signal} signal for {pair} at {entry_price}\")\n                \n                # For demonstration purposes, we'll simulate some trade outcomes\n                # In a real system, this would be updated when trades actually close\n                self._simulate_trade_outcome(len(self.performance_data['trades']) - 1)\n                \n        except Exception as e:\n            logger.error(f\"Error recording signal: {str(e)}\")\n    \n    def _simulate_trade_outcome(self, trade_index):\n        \"\"\"\n        Simulate trade outcomes for demonstration (replace with real trade tracking)\n        \"\"\"\n        try:\n            trade = self.performance_data['trades'][trade_index]\n            \n            # Simple simulation: 65% win rate based on confidence\n            import random\n            confidence_pct = trade['confidence'].get('percentage', 50)\n            \n            # Higher confidence = higher win probability\n            win_probability = min(0.85, confidence_pct / 100 + 0.15)\n            \n            if random.random() < win_probability:\n                # Winning trade\n                trade['result'] = 'WIN'\n                trade['exit_price'] = trade['tp_price']\n                if trade['signal'] == 'BUY':\n                    trade['profit_loss'] = trade['tp_price'] - trade['entry_price']\n                else:\n                    trade['profit_loss'] = trade['entry_price'] - trade['tp_price']\n            else:\n                # Losing trade\n                trade['result'] = 'LOSS'\n                trade['exit_price'] = trade['sl_price']\n                if trade['signal'] == 'BUY':\n                    trade['profit_loss'] = trade['sl_price'] - trade['entry_price']\n                else:\n                    trade['profit_loss'] = trade['entry_price'] - trade['sl_price']\n            \n            trade['status'] = 'CLOSED'\n            self._update_statistics()\n            self._save_performance_data()\n            \n        except Exception as e:\n            logger.error(f\"Error simulating trade outcome: {str(e)}\")\n    \n    def _update_statistics(self):\n        \"\"\"Update performance statistics\"\"\"\n        try:\n            trades = [t for t in self.performance_data['trades'] if t['status'] == 'CLOSED']\n            \n            if not trades:\n                return\n            \n            stats = self.performance_data['statistics']\n            \n            # Basic counts\n            stats['total_trades'] = len(trades)\n            winning_trades = [t for t in trades if t['result'] == 'WIN']\n            losing_trades = [t for t in trades if t['result'] == 'LOSS']\n            \n            stats['winning_trades'] = len(winning_trades)\n            stats['losing_trades'] = len(losing_trades)\n            \n            # Win rate\n            stats['win_rate'] = (stats['winning_trades'] / stats['total_trades']) * 100 if stats['total_trades'] > 0 else 0\n            \n            # Profit/Loss calculations\n            profits = [t['profit_loss'] for t in winning_trades]\n            losses = [abs(t['profit_loss']) for t in losing_trades]\n            \n            stats['total_profit'] = sum(profits) if profits else 0\n            stats['total_loss'] = sum(losses) if losses else 0\n            \n            # Profit factor\n            stats['profit_factor'] = (stats['total_profit'] / stats['total_loss']) if stats['total_loss'] > 0 else 0\n            \n            # Largest win/loss\n            stats['largest_win'] = max(profits) if profits else 0\n            stats['largest_loss'] = max(losses) if losses else 0\n            \n            # Average win/loss\n            stats['avg_win'] = sum(profits) / len(profits) if profits else 0\n            stats['avg_loss'] = sum(losses) / len(losses) if losses else 0\n            \n            # Streak calculations\n            current_streak = 0\n            best_streak = 0\n            worst_streak = 0\n            temp_streak = 0\n            \n            for trade in reversed(trades):\n                if trade['result'] == 'WIN':\n                    if temp_streak >= 0:\n                        temp_streak += 1\n                    else:\n                        temp_streak = 1\n                else:\n                    if temp_streak <= 0:\n                        temp_streak -= 1\n                    else:\n                        temp_streak = -1\n                \n                best_streak = max(best_streak, temp_streak)\n                worst_streak = min(worst_streak, temp_streak)\n            \n            current_streak = temp_streak\n            \n            stats['current_streak'] = current_streak\n            stats['best_streak'] = best_streak\n            stats['worst_streak'] = worst_streak\n            \n        except Exception as e:\n            logger.error(f\"Error updating statistics: {str(e)}\")\n    \n    def get_performance_summary(self):\n        \"\"\"Get current performance summary\"\"\"\n        try:\n            stats = self.performance_data['statistics']\n            recent_trades = self._get_recent_trades(limit=10)\n            \n            return {\n                'summary': {\n                    'total_trades': stats['total_trades'],\n                    'win_rate': round(stats['win_rate'], 1),\n                    'profit_factor': round(stats['profit_factor'], 2),\n                    'current_streak': stats['current_streak'],\n                    'largest_win': round(stats['largest_win'] * 10000, 1),  # Convert to pips\n                    'largest_loss': round(stats['largest_loss'] * 10000, 1)\n                },\n                'streak_info': {\n                    'current': stats['current_streak'],\n                    'best': stats['best_streak'],\n                    'worst': stats['worst_streak']\n                },\n                'recent_trades': recent_trades,\n                'last_updated': self.performance_data.get('last_updated'),\n                'system_age_days': self._get_system_age_days()\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error getting performance summary: {str(e)}\")\n            return {\n                'summary': {'total_trades': 0, 'win_rate': 0, 'profit_factor': 0, 'current_streak': 0},\n                'streak_info': {'current': 0, 'best': 0, 'worst': 0},\n                'recent_trades': [],\n                'error': str(e)\n            }\n    \n    def _get_recent_trades(self, limit=10):\n        \"\"\"Get recent closed trades\"\"\"\n        try:\n            closed_trades = [t for t in self.performance_data['trades'] if t['status'] == 'CLOSED']\n            recent = closed_trades[-limit:] if closed_trades else []\n            \n            # Format for display\n            formatted_trades = []\n            for trade in recent:\n                formatted_trades.append({\n                    'pair': trade['pair'],\n                    'signal': trade['signal'],\n                    'result': trade['result'],\n                    'profit_pips': round(trade['profit_loss'] * 10000, 1),\n                    'confidence': trade['confidence'].get('percentage', 0),\n                    'timestamp': trade['timestamp'][:10]  # Just date\n                })\n            \n            return formatted_trades\n            \n        except Exception as e:\n            logger.error(f\"Error getting recent trades: {str(e)}\")\n            return []\n    \n    def _get_system_age_days(self):\n        \"\"\"Calculate how many days the system has been running\"\"\"\n        try:\n            start_date_str = self.performance_data.get('system_start_date')\n            if start_date_str:\n                start_date = datetime.fromisoformat(start_date_str)\n                age = (datetime.now() - start_date).days\n                return max(1, age)  # At least 1 day\n            return 1\n        except:\n            return 1","size_bytes":11193},"static/css/dashboard.css":{"content":"/* Forex Trading Dashboard Styles */\n\nbody {\n    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n    min-height: 100vh;\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n}\n\n.container-fluid {\n    padding: 20px;\n}\n\n.header-section {\n    margin-bottom: 20px;\n}\n\n.header-section .card {\n    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n    border: none;\n    color: white;\n}\n\n.header-section h1 {\n    font-weight: 600;\n    text-shadow: 0 2px 4px rgba(0,0,0,0.3);\n}\n\n.signal-card {\n    transition: all 0.3s ease;\n    border: none;\n    box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n}\n\n.signal-card:hover {\n    transform: translateY(-5px);\n    box-shadow: 0 8px 20px rgba(0,0,0,0.15);\n}\n\n.signal-card.bg-success {\n    border-left: 5px solid #28a745;\n}\n\n.signal-card.bg-danger {\n    border-left: 5px solid #dc3545;\n}\n\n.signal-card.bg-warning {\n    border-left: 5px solid #ffc107;\n}\n\n.card-header {\n    border-bottom: 2px solid rgba(0,0,0,0.1);\n    font-weight: 600;\n}\n\n.price {\n    font-size: 1.5rem;\n    font-weight: 700;\n    color: #2c3e50;\n    margin: 0;\n}\n\n.progress {\n    border-radius: 10px;\n    overflow: hidden;\n}\n\n.progress-bar {\n    transition: width 0.6s ease;\n}\n\n.model-votes .badge {\n    font-size: 0.75rem;\n    padding: 0.25rem 0.5rem;\n    margin-bottom: 0.25rem;\n}\n\n.table-sm td {\n    padding: 0.3rem;\n    border-top: 1px solid rgba(0,0,0,0.1);\n    font-size: 0.9rem;\n}\n\n.btn-primary {\n    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n    border: none;\n    padding: 10px 20px;\n    font-weight: 600;\n    transition: all 0.3s ease;\n}\n\n.btn-primary:hover {\n    transform: translateY(-2px);\n    box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);\n}\n\n.spinner-border {\n    width: 3rem;\n    height: 3rem;\n}\n\n/* Mobile Responsiveness */\n@media (max-width: 768px) {\n    .container-fluid {\n        padding: 10px;\n    }\n    \n    .price {\n        font-size: 1.2rem;\n    }\n    \n    .card-body {\n        padding: 1rem;\n    }\n    \n    .table-sm {\n        font-size: 0.8rem;\n    }\n    \n    .model-votes .badge {\n        font-size: 0.65rem;\n        margin-bottom: 0.15rem;\n    }\n}\n\n@media (max-width: 576px) {\n    .header-section h1 {\n        font-size: 1.5rem;\n    }\n    \n    .price {\n        font-size: 1.1rem;\n    }\n    \n    .progress {\n        height: 15px !important;\n    }\n    \n    .progress-bar {\n        font-size: 0.7rem;\n    }\n    \n    .btn-primary {\n        padding: 8px 15px;\n        font-size: 0.9rem;\n    }\n}\n\n/* Animation for loading states */\n.fade-in {\n    animation: fadeIn 0.5s ease-in;\n}\n\n@keyframes fadeIn {\n    from {\n        opacity: 0;\n        transform: translateY(20px);\n    }\n    to {\n        opacity: 1;\n        transform: translateY(0);\n    }\n}\n\n/* Signal strength indicators */\n.signal-strength-high {\n    background: linear-gradient(45deg, #28a745, #20c997);\n}\n\n.signal-strength-medium {\n    background: linear-gradient(45deg, #ffc107, #fd7e14);\n}\n\n.signal-strength-low {\n    background: linear-gradient(45deg, #6c757d, #495057);\n}\n\n/* Enhanced Signal Cards */\n.enhanced-summary {\n    border-left: 4px solid #007bff;\n    font-size: 0.9rem;\n    line-height: 1.4;\n}\n\n.enhanced-summary p {\n    margin-bottom: 0.5rem;\n}\n\n/* Tab styling for signal analysis */\n.nav-tabs .nav-link {\n    border: none;\n    color: #6c757d;\n    font-weight: 500;\n    padding: 0.5rem 1rem;\n}\n\n.nav-tabs .nav-link.active {\n    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n    color: white;\n    border-radius: 5px;\n}\n\n.tab-content {\n    min-height: 200px;\n    padding: 1rem;\n    background: rgba(0,0,0,0.02);\n    border-radius: 5px;\n}\n\n/* Performance metrics */\n.performance-card {\n    transition: transform 0.3s ease;\n}\n\n.performance-card:hover {\n    transform: translateY(-2px);\n}\n\n/* Market regime indicators */\n.regime-trending {\n    color: #28a745;\n    font-weight: bold;\n}\n\n.regime-ranging {\n    color: #ffc107;\n    font-weight: bold;\n}\n\n.regime-volatile {\n    color: #dc3545;\n    font-weight: bold;\n}\n\n/* Confidence levels */\n.confidence-confirmed {\n    background: linear-gradient(45deg, #28a745, #20c997) !important;\n}\n\n.confidence-likely {\n    background: linear-gradient(45deg, #17a2b8, #6f42c1) !important;\n}\n\n.confidence-possible {\n    background: linear-gradient(45deg, #ffc107, #fd7e14) !important;\n}\n\n.confidence-weak {\n    background: linear-gradient(45deg, #6c757d, #495057) !important;\n}\n\n/* Timeframe consensus badges */\n.timeframe-signals .badge {\n    margin: 0.2rem;\n    padding: 0.4rem 0.6rem;\n    font-size: 0.75rem;\n}\n\n/* Price action warnings */\n.price-action-warning {\n    background: rgba(255, 193, 7, 0.1);\n    border: 1px solid #ffc107;\n    border-radius: 5px;\n    padding: 0.5rem;\n}\n\n/* ATR display */\n.atr-info {\n    background: rgba(23, 162, 184, 0.1);\n    border: 1px solid #17a2b8;\n    border-radius: 5px;\n    padding: 0.75rem;\n}\n\n/* Performance dashboard */\n#performanceSection .card {\n    box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n    border: none;\n}\n\n#performanceSection .card-body {\n    padding: 1.5rem;\n}\n\n#performanceSection h4 {\n    font-size: 2rem;\n    font-weight: 700;\n    margin-bottom: 0.5rem;\n}\n\n#performanceSection p {\n    margin-bottom: 0;\n    font-size: 0.9rem;\n    opacity: 0.9;\n}\n\n/* Enhanced mobile responsiveness */\n@media (max-width: 768px) {\n    .enhanced-summary {\n        font-size: 0.8rem;\n    }\n    \n    .nav-tabs .nav-link {\n        padding: 0.4rem 0.8rem;\n        font-size: 0.85rem;\n    }\n    \n    .tab-content {\n        padding: 0.75rem;\n        min-height: 150px;\n    }\n    \n    #performanceSection h4 {\n        font-size: 1.5rem;\n    }\n    \n    .timeframe-signals .badge {\n        font-size: 0.65rem;\n        padding: 0.25rem 0.4rem;\n    }\n}\n\n@media (max-width: 576px) {\n    .enhanced-summary {\n        font-size: 0.75rem;\n        padding: 0.75rem;\n    }\n    \n    .nav-tabs .nav-link {\n        padding: 0.3rem 0.6rem;\n        font-size: 0.8rem;\n    }\n    \n    .tab-content {\n        padding: 0.5rem;\n        min-height: 120px;\n    }\n    \n    #performanceSection .card-body {\n        padding: 1rem;\n    }\n    \n    #performanceSection h4 {\n        font-size: 1.25rem;\n    }\n    \n    .table-sm {\n        font-size: 0.75rem;\n    }\n}\n\n/* Custom scrollbar for mobile */\n::-webkit-scrollbar {\n    width: 8px;\n}\n\n::-webkit-scrollbar-track {\n    background: #f1f1f1;\n    border-radius: 10px;\n}\n\n::-webkit-scrollbar-thumb {\n    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n    border-radius: 10px;\n}\n\n::-webkit-scrollbar-thumb:hover {\n    background: linear-gradient(135deg, #764ba2 0%, #667eea 100%);\n}","size_bytes":6543},"replit.md":{"content":"# Forex Trading System\n\n## Overview\n\nThis is a Flask-based forex trading analysis system that provides real-time trading signals for EUR/USD and USD/JPY currency pairs. The system uses a 5-model ensemble approach to analyze market conditions and generate trading recommendations, combined with comprehensive risk management features. The application fetches live forex data, applies multiple technical analysis models, and calculates position sizing with stop-loss and take-profit levels.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## System Architecture\n\n### Frontend Architecture\n- **Framework**: Flask with Jinja2 templating\n- **UI Framework**: Bootstrap 5.1.3 for responsive design\n- **Icons**: Font Awesome 6.0.0 for visual elements\n- **JavaScript**: Vanilla JavaScript for API interactions and dynamic updates\n- **CSS**: Custom gradient-based styling with hover effects and responsive layouts\n\n### Backend Architecture\n- **Web Framework**: Flask application with modular trading components\n- **API Design**: RESTful endpoints for signal generation and data retrieval\n- **Session Management**: Flask session handling with configurable secret key\n- **Error Handling**: Comprehensive exception handling with logging throughout the system\n\n### Trading System Components\n- **Data Fetcher**: Real-time forex data retrieval using yfinance library with caching mechanism\n- **Ensemble Analyzer**: 5-model technical analysis system including:\n  - SMA Crossover strategy\n  - RSI analysis\n  - MACD analysis  \n  - Bollinger Bands strategy\n  - Price Momentum analysis\n- **Risk Manager**: Position sizing and risk calculation with configurable stop-loss and take-profit levels\n\n### Data Processing\n- **Technical Analysis**: Uses TA-Lib library for technical indicator calculations\n- **Data Validation**: Input sanitization and data quality checks\n- **Caching Strategy**: 5-minute cache duration for forex data to optimize API usage\n- **Signal Aggregation**: Weighted voting system across multiple models for final recommendations\n\n### Risk Management\n- **Position Sizing**: Dynamic lot size calculation based on account size and risk percentage\n- **Stop Loss/Take Profit**: Pair-specific pip calculations with 2:1 risk-reward ratios\n- **Account Configuration**: User-configurable account size and risk tolerance settings\n\n## External Dependencies\n\n### Data Sources\n- **yfinance**: Primary data source for real-time forex prices and historical data\n- **Yahoo Finance API**: Underlying data provider for currency pair information\n\n### Technical Analysis\n- **TA-Lib**: Technical analysis library for indicator calculations\n- **pandas**: Data manipulation and time series analysis\n- **numpy**: Numerical computations and array operations\n\n### Web Framework\n- **Flask**: Core web application framework\n- **Bootstrap CDN**: Frontend styling and responsive components\n- **Font Awesome CDN**: Icon library for user interface elements\n\n### Python Libraries\n- **datetime**: Time-based operations and data timestamping\n- **logging**: Application logging and error tracking\n- **traceback**: Error diagnosis and debugging support\n\n### Environment Configuration\n- **SESSION_SECRET**: Environment variable for Flask session security\n- **Default Configuration**: Fallback values for account size ($10,000) and risk percentage (1.5%)","size_bytes":3345},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"flask-sqlalchemy>=3.1.1\",\n    \"flask>=3.1.2\",\n    \"gunicorn>=23.0.0\",\n    \"numpy>=2.3.3\",\n    \"pandas>=2.3.2\",\n    \"ta-lib>=0.6.7\",\n    \"yfinance>=0.2.66\",\n    \"psycopg2-binary>=2.9.10\",\n    \"email-validator>=2.3.0\",\n    \"scikit-learn>=1.7.2\",\n    \"xgboost>=3.0.5\",\n    \"scipy>=1.16.2\",\n    \"statsmodels>=0.14.5\",\n    \"arch>=7.2.0\",\n]\n","size_bytes":482},"trading/backtester.py":{"content":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport logging\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nimport json\nimport os\nfrom typing import Dict, List, Tuple, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass InstitutionalBacktester:\n    \"\"\"Institutional-Grade Backtesting with Walk-Forward Analysis and Triple-Barrier Labeling\"\"\"\n    \n    def __init__(self, ensemble_analyzer, risk_manager):\n        self.ensemble_analyzer = ensemble_analyzer\n        self.risk_manager = risk_manager\n        \n        # Triple-barrier parameters for intraday trading\n        self.barrier_config = {\n            '5m': {'profit_barrier': 0.0015, 'stop_barrier': 0.0008, 'time_barrier': 12},  # 1 hour\n            '15m': {'profit_barrier': 0.0025, 'stop_barrier': 0.0012, 'time_barrier': 8},   # 2 hours\n            '1h': {'profit_barrier': 0.004, 'stop_barrier': 0.002, 'time_barrier': 6}       # 6 hours\n        }\n        \n        # Walk-forward parameters\n        self.walk_forward_config = {\n            'initial_window': 500,  # Initial training window\n            'step_size': 50,        # Step size for walk-forward\n            'min_trades': 20        # Minimum trades for valid period\n        }\n        \n        # Performance tracking\n        self.backtest_results = {}\n        self.detailed_trades = []\n        \n    def run_walk_forward_backtest(self, data: pd.DataFrame, pair: str, timeframe: str = '15m', \n                                account_size: float = 10000) -> Dict:\n        \"\"\"\n        Run comprehensive walk-forward backtest with institutional metrics\n        \n        Args:\n            data: OHLCV data for backtesting\n            pair: Currency pair (e.g., 'EURUSD')\n            timeframe: Trading timeframe\n            account_size: Account size for position sizing\n            \n        Returns:\n            Comprehensive backtest results with institutional metrics\n        \"\"\"\n        try:\n            if len(data) < self.walk_forward_config['initial_window'] + 100:\n                logger.warning(f\"Insufficient data for walk-forward backtest: {len(data)} rows\")\n                return self._default_backtest_results()\n            \n            logger.info(f\"Starting walk-forward backtest for {pair} on {timeframe}\")\n            \n            # Prepare data\n            data = data.sort_index().copy()\n            \n            # Initialize results tracking\n            all_trades = []\n            period_results = []\n            \n            # Walk-forward analysis\n            initial_window = self.walk_forward_config['initial_window']\n            step_size = self.walk_forward_config['step_size']\n            \n            for start_idx in range(initial_window, len(data) - 100, step_size):\n                end_idx = min(start_idx + step_size, len(data))\n                \n                # Training data (for model calibration)\n                train_data = data.iloc[start_idx - initial_window:start_idx]\n                \n                # Test data (for trading simulation)\n                test_data = data.iloc[start_idx:end_idx]\n                \n                # Run period backtest\n                period_trades = self._backtest_period(\n                    train_data, test_data, pair, timeframe, account_size\n                )\n                \n                if len(period_trades) >= self.walk_forward_config['min_trades']:\n                    all_trades.extend(period_trades)\n                    \n                    # Calculate period metrics\n                    period_metrics = self._calculate_period_metrics(period_trades)\n                    period_metrics['period_start'] = test_data.index[0]\n                    period_metrics['period_end'] = test_data.index[-1]\n                    period_results.append(period_metrics)\n            \n            # Calculate comprehensive results\n            if len(all_trades) > 0:\n                results = self._calculate_comprehensive_results(\n                    all_trades, period_results, pair, timeframe, account_size\n                )\n                \n                # Save detailed results\n                self._save_backtest_results(results, pair, timeframe)\n                \n                logger.info(f\"Backtest completed: {len(all_trades)} trades, \"\n                          f\"Win rate: {results['performance_metrics']['win_rate']:.1f}%\")\n                \n                return results\n            else:\n                logger.warning(\"No valid trades generated in backtest\")\n                return self._default_backtest_results()\n                \n        except Exception as e:\n            logger.error(f\"Error in walk-forward backtest: {str(e)}\")\n            return self._default_backtest_results()\n    \n    def _backtest_period(self, train_data: pd.DataFrame, test_data: pd.DataFrame, \n                        pair: str, timeframe: str, account_size: float) -> List[Dict]:\n        \"\"\"Backtest a specific period with triple-barrier labeling\"\"\"\n        try:\n            trades = []\n            \n            for i in range(len(test_data) - 50):  # Leave buffer for barriers\n                current_data = pd.concat([train_data, test_data.iloc[:i+1]])\n                \n                if len(current_data) < 100:\n                    continue\n                \n                # Get institutional signal\n                analysis = self.ensemble_analyzer.analyze(current_data)\n                \n                if analysis['final_signal'] in ['BUY', 'SELL']:\n                    # Create trade with triple-barrier labeling\n                    trade = self._create_triple_barrier_trade(\n                        test_data, i, analysis, pair, timeframe, account_size\n                    )\n                    \n                    if trade:\n                        trades.append(trade)\n            \n            return trades\n            \n        except Exception as e:\n            logger.warning(f\"Error in period backtest: {str(e)}\")\n            return []\n    \n    def _create_triple_barrier_trade(self, data: pd.DataFrame, entry_idx: int, \n                                   analysis: Dict, pair: str, timeframe: str, \n                                   account_size: float) -> Optional[Dict]:\n        \"\"\"Create trade with triple-barrier exit logic\"\"\"\n        try:\n            if entry_idx >= len(data) - 20:  # Need buffer for barriers\n                return None\n            \n            entry_price = data['Close'].iloc[entry_idx]\n            entry_time = data.index[entry_idx]\n            signal = analysis['final_signal']\n            \n            # Get barrier parameters\n            barriers = self.barrier_config.get(timeframe, self.barrier_config['15m'])\n            \n            # Calculate barrier levels\n            if signal == 'BUY':\n                profit_target = entry_price * (1 + barriers['profit_barrier'])\n                stop_loss = entry_price * (1 - barriers['stop_barrier'])\n            else:  # SELL\n                profit_target = entry_price * (1 - barriers['profit_barrier'])\n                stop_loss = entry_price * (1 + barriers['stop_barrier'])\n            \n            # Find exit point\n            exit_info = self._find_triple_barrier_exit(\n                data, entry_idx, profit_target, stop_loss, \n                barriers['time_barrier'], signal\n            )\n            \n            if not exit_info:\n                return None\n            \n            # Calculate trade results\n            if signal == 'BUY':\n                pnl_pips = (exit_info['exit_price'] - entry_price) / 0.0001  # Assuming major pair\n            else:\n                pnl_pips = (entry_price - exit_info['exit_price']) / 0.0001\n            \n            # Get position sizing from risk manager\n            risk_calc = self.risk_manager.calculate_risk_levels(\n                pair=pair,\n                current_price=entry_price,\n                account_size=account_size,\n                risk_percent=0.5,  # Conservative for backtesting\n                timeframe=timeframe,\n                kelly_fraction=analysis.get('kelly_fraction')\n            )\n            \n            position_size = risk_calc['position_size']['micro_lots']\n            pnl_usd = pnl_pips * position_size * 0.1  # Approximate USD per pip for micro lot\n            \n            return {\n                'entry_time': entry_time,\n                'exit_time': exit_info['exit_time'],\n                'signal': signal,\n                'entry_price': entry_price,\n                'exit_price': exit_info['exit_price'],\n                'exit_reason': exit_info['exit_reason'],\n                'pnl_pips': round(pnl_pips, 1),\n                'pnl_usd': round(pnl_usd, 2),\n                'position_size': position_size,\n                'profit_target': profit_target,\n                'stop_loss': stop_loss,\n                'confidence': analysis['confidence']['percentage'],\n                'expected_value': analysis.get('expected_value', 0),\n                'kelly_fraction': analysis.get('kelly_fraction', 0),\n                'market_regime': analysis['market_regime']['regime'],\n                'meta_decision': analysis.get('meta_decision', 1),\n                'filters_passed': analysis.get('filters_passed', {}),\n                'duration_bars': exit_info['duration_bars']\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error creating triple-barrier trade: {str(e)}\")\n            return None\n    \n    def _find_triple_barrier_exit(self, data: pd.DataFrame, entry_idx: int, \n                                 profit_target: float, stop_loss: float, \n                                 time_barrier: int, signal: str) -> Optional[Dict]:\n        \"\"\"Find exit point using triple-barrier logic\"\"\"\n        try:\n            max_idx = min(entry_idx + time_barrier, len(data) - 1)\n            \n            for i in range(entry_idx + 1, max_idx + 1):\n                current_high = data['High'].iloc[i]\n                current_low = data['Low'].iloc[i]\n                current_close = data['Close'].iloc[i]\n                \n                if signal == 'BUY':\n                    # Check profit target\n                    if current_high >= profit_target:\n                        return {\n                            'exit_price': profit_target,\n                            'exit_time': data.index[i],\n                            'exit_reason': 'profit_target',\n                            'duration_bars': i - entry_idx\n                        }\n                    # Check stop loss\n                    if current_low <= stop_loss:\n                        return {\n                            'exit_price': stop_loss,\n                            'exit_time': data.index[i],\n                            'exit_reason': 'stop_loss',\n                            'duration_bars': i - entry_idx\n                        }\n                else:  # SELL\n                    # Check profit target\n                    if current_low <= profit_target:\n                        return {\n                            'exit_price': profit_target,\n                            'exit_time': data.index[i],\n                            'exit_reason': 'profit_target',\n                            'duration_bars': i - entry_idx\n                        }\n                    # Check stop loss\n                    if current_high >= stop_loss:\n                        return {\n                            'exit_price': stop_loss,\n                            'exit_time': data.index[i],\n                            'exit_reason': 'stop_loss',\n                            'duration_bars': i - entry_idx\n                        }\n            \n            # Time barrier hit\n            return {\n                'exit_price': data['Close'].iloc[max_idx],\n                'exit_time': data.index[max_idx],\n                'exit_reason': 'time_barrier',\n                'duration_bars': max_idx - entry_idx\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error finding triple-barrier exit: {str(e)}\")\n            return None\n    \n    def _calculate_period_metrics(self, trades: List[Dict]) -> Dict:\n        \"\"\"Calculate performance metrics for a specific period\"\"\"\n        try:\n            if not trades:\n                return {'trades': 0, 'win_rate': 0, 'profit_factor': 0, 'sharpe_ratio': 0}\n            \n            winning_trades = [t for t in trades if t['pnl_usd'] > 0]\n            losing_trades = [t for t in trades if t['pnl_usd'] < 0]\n            \n            total_profit = sum(t['pnl_usd'] for t in winning_trades)\n            total_loss = abs(sum(t['pnl_usd'] for t in losing_trades))\n            \n            win_rate = len(winning_trades) / len(trades) * 100\n            profit_factor = total_profit / total_loss if total_loss > 0 else float('inf')\n            \n            # Calculate Sharpe ratio\n            returns = [t['pnl_usd'] for t in trades]\n            sharpe_ratio = np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0\n            \n            return {\n                'trades': len(trades),\n                'winning_trades': len(winning_trades),\n                'losing_trades': len(losing_trades),\n                'win_rate': round(win_rate, 1),\n                'profit_factor': round(profit_factor, 2),\n                'total_profit': round(total_profit, 2),\n                'total_loss': round(total_loss, 2),\n                'net_profit': round(total_profit - total_loss, 2),\n                'sharpe_ratio': round(sharpe_ratio, 2),\n                'avg_win': round(total_profit / len(winning_trades), 2) if winning_trades else 0,\n                'avg_loss': round(total_loss / len(losing_trades), 2) if losing_trades else 0\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error calculating period metrics: {str(e)}\")\n            return {'trades': 0, 'win_rate': 0, 'profit_factor': 0, 'sharpe_ratio': 0}\n    \n    def _calculate_comprehensive_results(self, all_trades: List[Dict], \n                                       period_results: List[Dict], pair: str, \n                                       timeframe: str, account_size: float) -> Dict:\n        \"\"\"Calculate comprehensive institutional-grade results\"\"\"\n        try:\n            # Overall performance metrics\n            overall_metrics = self._calculate_period_metrics(all_trades)\n            \n            # Confidence-based analysis\n            confidence_analysis = self._analyze_by_confidence(all_trades)\n            \n            # Regime-based analysis\n            regime_analysis = self._analyze_by_regime(all_trades)\n            \n            # Expected Value analysis\n            ev_analysis = self._analyze_expected_value(all_trades)\n            \n            # Meta-labeling effectiveness\n            meta_analysis = self._analyze_meta_labeling(all_trades)\n            \n            # Risk metrics\n            risk_metrics = self._calculate_risk_metrics(all_trades, account_size)\n            \n            # Trade frequency and expectancy\n            frequency_metrics = self._calculate_frequency_metrics(all_trades, timeframe)\n            \n            return {\n                'backtest_info': {\n                    'pair': pair,\n                    'timeframe': timeframe,\n                    'account_size': account_size,\n                    'total_periods': len(period_results),\n                    'backtest_date': datetime.now().isoformat(),\n                    'walk_forward_config': self.walk_forward_config,\n                    'barrier_config': self.barrier_config[timeframe]\n                },\n                'performance_metrics': overall_metrics,\n                'confidence_distribution': confidence_analysis,\n                'regime_performance': regime_analysis,\n                'expected_value_analysis': ev_analysis,\n                'meta_labeling_analysis': meta_analysis,\n                'risk_metrics': risk_metrics,\n                'frequency_metrics': frequency_metrics,\n                'period_results': period_results,\n                'detailed_trades': all_trades[-100:],  # Last 100 trades for analysis\n                'summary': {\n                    'recommendation': self._generate_recommendation(overall_metrics, risk_metrics),\n                    'key_insights': self._generate_insights(all_trades, overall_metrics)\n                }\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error calculating comprehensive results: {str(e)}\")\n            return self._default_backtest_results()\n    \n    def _analyze_by_confidence(self, trades: List[Dict]) -> Dict:\n        \"\"\"Analyze performance by confidence buckets\"\"\"\n        try:\n            buckets = {'60-70%': [], '70-80%': [], '80-90%': [], '90-100%': []}\n            \n            for trade in trades:\n                confidence = trade.get('confidence', 50)\n                if 60 <= confidence < 70:\n                    buckets['60-70%'].append(trade)\n                elif 70 <= confidence < 80:\n                    buckets['70-80%'].append(trade)\n                elif 80 <= confidence < 90:\n                    buckets['80-90%'].append(trade)\n                elif confidence >= 90:\n                    buckets['90-100%'].append(trade)\n            \n            analysis = {}\n            for bucket, bucket_trades in buckets.items():\n                if bucket_trades:\n                    metrics = self._calculate_period_metrics(bucket_trades)\n                    analysis[bucket] = {\n                        'trades': len(bucket_trades),\n                        'win_rate': metrics['win_rate'],\n                        'avg_pnl': round(np.mean([t['pnl_usd'] for t in bucket_trades]), 2),\n                        'sharpe_ratio': metrics['sharpe_ratio']\n                    }\n                else:\n                    analysis[bucket] = {'trades': 0, 'win_rate': 0, 'avg_pnl': 0, 'sharpe_ratio': 0}\n            \n            return analysis\n            \n        except Exception as e:\n            logger.warning(f\"Error analyzing by confidence: {str(e)}\")\n            return {}\n    \n    def _analyze_by_regime(self, trades: List[Dict]) -> Dict:\n        \"\"\"Analyze performance by market regime\"\"\"\n        try:\n            regimes = {}\n            \n            for trade in trades:\n                regime = trade.get('market_regime', 'UNKNOWN')\n                if regime not in regimes:\n                    regimes[regime] = []\n                regimes[regime].append(trade)\n            \n            analysis = {}\n            for regime, regime_trades in regimes.items():\n                if regime_trades:\n                    metrics = self._calculate_period_metrics(regime_trades)\n                    analysis[regime] = {\n                        'trades': len(regime_trades),\n                        'win_rate': metrics['win_rate'],\n                        'profit_factor': metrics['profit_factor'],\n                        'avg_pnl': round(np.mean([t['pnl_usd'] for t in regime_trades]), 2)\n                    }\n            \n            return analysis\n            \n        except Exception as e:\n            logger.warning(f\"Error analyzing by regime: {str(e)}\")\n            return {}\n    \n    def _analyze_expected_value(self, trades: List[Dict]) -> Dict:\n        \"\"\"Analyze Expected Value effectiveness\"\"\"\n        try:\n            ev_positive = [t for t in trades if t.get('expected_value', 0) > 0]\n            ev_negative = [t for t in trades if t.get('expected_value', 0) <= 0]\n            \n            return {\n                'positive_ev': {\n                    'trades': len(ev_positive),\n                    'win_rate': self._calculate_period_metrics(ev_positive)['win_rate'] if ev_positive else 0,\n                    'avg_pnl': round(np.mean([t['pnl_usd'] for t in ev_positive]), 2) if ev_positive else 0\n                },\n                'negative_ev': {\n                    'trades': len(ev_negative),\n                    'win_rate': self._calculate_period_metrics(ev_negative)['win_rate'] if ev_negative else 0,\n                    'avg_pnl': round(np.mean([t['pnl_usd'] for t in ev_negative]), 2) if ev_negative else 0\n                }\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error analyzing expected value: {str(e)}\")\n            return {}\n    \n    def _analyze_meta_labeling(self, trades: List[Dict]) -> Dict:\n        \"\"\"Analyze meta-labeling effectiveness\"\"\"\n        try:\n            meta_approved = [t for t in trades if t.get('meta_decision', 1) == 1]\n            meta_rejected = [t for t in trades if t.get('meta_decision', 1) == 0]\n            \n            return {\n                'meta_approved': {\n                    'trades': len(meta_approved),\n                    'win_rate': self._calculate_period_metrics(meta_approved)['win_rate'] if meta_approved else 0,\n                    'avg_pnl': round(np.mean([t['pnl_usd'] for t in meta_approved]), 2) if meta_approved else 0\n                },\n                'meta_rejected': {\n                    'trades': len(meta_rejected),\n                    'win_rate': self._calculate_period_metrics(meta_rejected)['win_rate'] if meta_rejected else 0,\n                    'avg_pnl': round(np.mean([t['pnl_usd'] for t in meta_rejected]), 2) if meta_rejected else 0\n                },\n                'effectiveness': len(meta_approved) / (len(meta_approved) + len(meta_rejected)) * 100 if trades else 0\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error analyzing meta-labeling: {str(e)}\")\n            return {}\n    \n    def _calculate_risk_metrics(self, trades: List[Dict], account_size: float) -> Dict:\n        \"\"\"Calculate comprehensive risk metrics\"\"\"\n        try:\n            if not trades:\n                return {}\n            \n            returns = [t['pnl_usd'] / account_size for t in trades]\n            cumulative_returns = np.cumprod(1 + np.array(returns))\n            \n            # Maximum Drawdown\n            peak = np.maximum.accumulate(cumulative_returns)\n            drawdown = (cumulative_returns - peak) / peak\n            max_drawdown = abs(np.min(drawdown))\n            \n            # Calmar Ratio\n            total_return = cumulative_returns[-1] - 1\n            calmar_ratio = total_return / max_drawdown if max_drawdown > 0 else 0\n            \n            # Sortino Ratio\n            negative_returns = [r for r in returns if r < 0]\n            downside_deviation = np.std(negative_returns) if negative_returns else 0\n            sortino_ratio = np.mean(returns) / downside_deviation if downside_deviation > 0 else 0\n            \n            # VaR and CVaR\n            var_95 = np.percentile(returns, 5)\n            cvar_95 = np.mean([r for r in returns if r <= var_95])\n            \n            return {\n                'max_drawdown': round(max_drawdown, 4),\n                'calmar_ratio': round(calmar_ratio, 2),\n                'sortino_ratio': round(sortino_ratio, 2),\n                'var_95': round(var_95, 4),\n                'cvar_95': round(cvar_95, 4),\n                'volatility': round(np.std(returns), 4),\n                'skewness': round(float(pd.Series(returns).skew()), 2),\n                'kurtosis': round(float(pd.Series(returns).kurtosis()), 2)\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error calculating risk metrics: {str(e)}\")\n            return {}\n    \n    def _calculate_frequency_metrics(self, trades: List[Dict], timeframe: str) -> Dict:\n        \"\"\"Calculate trade frequency and expectancy metrics\"\"\"\n        try:\n            if not trades:\n                return {}\n            \n            # Calculate time span\n            start_time = min(trade['entry_time'] for trade in trades)\n            end_time = max(trade['exit_time'] for trade in trades)\n            total_hours = (end_time - start_time).total_seconds() / 3600\n            \n            # Trade frequency\n            trades_per_day = len(trades) / (total_hours / 24) if total_hours > 0 else 0\n            \n            # Average trade duration\n            avg_duration = np.mean([t['duration_bars'] for t in trades])\n            \n            # Expectancy\n            win_rate = len([t for t in trades if t['pnl_usd'] > 0]) / len(trades)\n            avg_win = np.mean([t['pnl_usd'] for t in trades if t['pnl_usd'] > 0])\n            avg_loss = abs(np.mean([t['pnl_usd'] for t in trades if t['pnl_usd'] < 0]))\n            expectancy = (win_rate * avg_win) - ((1 - win_rate) * avg_loss)\n            \n            return {\n                'trades_per_day': round(trades_per_day, 1),\n                'avg_duration_bars': round(avg_duration, 1),\n                'expectancy_per_trade': round(expectancy, 2),\n                'expectancy_per_day': round(expectancy * trades_per_day, 2),\n                'total_trading_days': round(total_hours / 24, 1)\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error calculating frequency metrics: {str(e)}\")\n            return {}\n    \n    def _generate_recommendation(self, performance: Dict, risk: Dict) -> str:\n        \"\"\"Generate trading recommendation based on backtest results\"\"\"\n        try:\n            win_rate = performance.get('win_rate', 0)\n            profit_factor = performance.get('profit_factor', 0)\n            sharpe_ratio = performance.get('sharpe_ratio', 0)\n            max_drawdown = risk.get('max_drawdown', 1)\n            \n            if win_rate >= 60 and profit_factor >= 1.5 and sharpe_ratio >= 1.0 and max_drawdown <= 0.15:\n                return \"STRONG BUY - Excellent performance across all metrics\"\n            elif win_rate >= 55 and profit_factor >= 1.2 and sharpe_ratio >= 0.5 and max_drawdown <= 0.25:\n                return \"BUY - Good performance with acceptable risk\"\n            elif win_rate >= 50 and profit_factor >= 1.0 and max_drawdown <= 0.35:\n                return \"NEUTRAL - Mixed performance, requires optimization\"\n            else:\n                return \"AVOID - Poor performance or excessive risk\"\n                \n        except Exception:\n            return \"INSUFFICIENT DATA - Unable to generate recommendation\"\n    \n    def _generate_insights(self, trades: List[Dict], performance: Dict) -> List[str]:\n        \"\"\"Generate key insights from backtest results\"\"\"\n        insights = []\n        \n        try:\n            # Performance insights\n            win_rate = performance.get('win_rate', 0)\n            if win_rate > 65:\n                insights.append(f\"Excellent win rate of {win_rate}% indicates strong signal quality\")\n            elif win_rate < 45:\n                insights.append(f\"Low win rate of {win_rate}% suggests signal optimization needed\")\n            \n            # Exit reason analysis\n            exit_reasons = {}\n            for trade in trades:\n                reason = trade.get('exit_reason', 'unknown')\n                exit_reasons[reason] = exit_reasons.get(reason, 0) + 1\n            \n            total_exits = sum(exit_reasons.values())\n            if total_exits > 0:\n                for reason, count in exit_reasons.items():\n                    pct = (count / total_exits) * 100\n                    if pct > 30:\n                        insights.append(f\"{pct:.0f}% of trades exit via {reason.replace('_', ' ')}\")\n            \n            # Regime performance\n            regime_trades = {}\n            for trade in trades:\n                regime = trade.get('market_regime', 'UNKNOWN')\n                if regime not in regime_trades:\n                    regime_trades[regime] = []\n                regime_trades[regime].append(trade['pnl_usd'])\n            \n            for regime, pnls in regime_trades.items():\n                if len(pnls) > 10:  # Sufficient sample size\n                    avg_pnl = np.mean(pnls)\n                    if avg_pnl > 5:\n                        insights.append(f\"Strong performance in {regime} markets (avg: ${avg_pnl:.1f})\")\n                    elif avg_pnl < -2:\n                        insights.append(f\"Weak performance in {regime} markets (avg: ${avg_pnl:.1f})\")\n            \n            return insights[:5]  # Return top 5 insights\n            \n        except Exception as e:\n            logger.warning(f\"Error generating insights: {str(e)}\")\n            return [\"Analysis completed successfully\"]\n    \n    def _save_backtest_results(self, results: Dict, pair: str, timeframe: str):\n        \"\"\"Save backtest results to file\"\"\"\n        try:\n            filename = f\"backtest_{pair}_{timeframe}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n            filepath = os.path.join('backtest_results', filename)\n            \n            # Create directory if it doesn't exist\n            os.makedirs('backtest_results', exist_ok=True)\n            \n            # Save results\n            with open(filepath, 'w') as f:\n                json.dump(results, f, indent=2, default=str)\n            \n            logger.info(f\"Backtest results saved to {filepath}\")\n            \n        except Exception as e:\n            logger.warning(f\"Error saving backtest results: {str(e)}\")\n    \n    def _default_backtest_results(self) -> Dict:\n        \"\"\"Return default backtest results when analysis fails\"\"\"\n        return {\n            'backtest_info': {\n                'pair': 'UNKNOWN',\n                'timeframe': 'UNKNOWN',\n                'status': 'FAILED',\n                'backtest_date': datetime.now().isoformat()\n            },\n            'performance_metrics': {\n                'trades': 0,\n                'win_rate': 0,\n                'profit_factor': 0,\n                'sharpe_ratio': 0,\n                'net_profit': 0\n            },\n            'confidence_distribution': {},\n            'regime_performance': {},\n            'risk_metrics': {},\n            'summary': {\n                'recommendation': 'INSUFFICIENT DATA',\n                'key_insights': ['Backtest failed - insufficient data or configuration error']\n            }\n        }\n    \n    def get_backtest_summary(self, pair: str = None, timeframe: str = None) -> Dict:\n        \"\"\"Get summary of all backtest results\"\"\"\n        try:\n            backtest_dir = 'backtest_results'\n            if not os.path.exists(backtest_dir):\n                return {'status': 'No backtest results found'}\n            \n            summaries = []\n            for filename in os.listdir(backtest_dir):\n                if filename.endswith('.json'):\n                    try:\n                        with open(os.path.join(backtest_dir, filename), 'r') as f:\n                            result = json.load(f)\n                            \n                        # Filter by pair and timeframe if specified\n                        if pair and result['backtest_info'].get('pair') != pair:\n                            continue\n                        if timeframe and result['backtest_info'].get('timeframe') != timeframe:\n                            continue\n                        \n                        summary = {\n                            'filename': filename,\n                            'pair': result['backtest_info'].get('pair'),\n                            'timeframe': result['backtest_info'].get('timeframe'),\n                            'date': result['backtest_info'].get('backtest_date'),\n                            'trades': result['performance_metrics'].get('trades', 0),\n                            'win_rate': result['performance_metrics'].get('win_rate', 0),\n                            'net_profit': result['performance_metrics'].get('net_profit', 0),\n                            'max_drawdown': result.get('risk_metrics', {}).get('max_drawdown', 0),\n                            'recommendation': result.get('summary', {}).get('recommendation', 'UNKNOWN')\n                        }\n                        summaries.append(summary)\n                        \n                    except Exception as e:\n                        logger.warning(f\"Error reading backtest file {filename}: {str(e)}\")\n                        continue\n            \n            return {\n                'status': 'success',\n                'total_backtests': len(summaries),\n                'summaries': sorted(summaries, key=lambda x: x['date'], reverse=True)\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error getting backtest summary: {str(e)}\")\n            return {'status': 'error', 'message': str(e)}","size_bytes":32592}},"version":2}